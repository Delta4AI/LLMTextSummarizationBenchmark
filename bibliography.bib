@article{chavesAutomaticTextSummarization2022,
  title = {Automatic {{Text Summarization}} of {{Biomedical Text Data}}: {{A Systematic Review}}},
  shorttitle = {Automatic {{Text Summarization}} of {{Biomedical Text Data}}},
  author = {Chaves, Andrea and Kesiku, Cyrille and {Garcia-Zapirain}, Begonya},
  year = {2022},
  month = aug,
  journal = {Information},
  volume = {13},
  number = {8},
  pages = {393},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2078-2489},
  doi = {10.3390/info13080393},
  urldate = {2025-08-04},
  abstract = {In recent years, the evolution of technology has led to an increase in text data obtained from many sources. In the biomedical domain, text information has also evidenced this accelerated growth, and automatic text summarization systems play an essential role in optimizing physicians' time resources and identifying relevant information. In this paper, we present a systematic review in recent research of text summarization for biomedical textual data, focusing mainly on the methods employed, type of input data text, areas of application, and evaluation metrics used to assess systems. The survey was limited to the period between 1st January 2014 and 15th March 2022. The data collected was obtained from WoS, IEEE, and ACM digital libraries, while the search strategies were developed with the help of experts in NLP techniques and previous systematic reviews. The four phases of a systematic review by PRISMA methodology were conducted, and five summarization factors were determined to assess the studies included: Input, Purpose, Output, Method, and Evaluation metric. Results showed that 3.5\% of 801 studies met the inclusion criteria. Moreover, Single-document, Biomedical Literature, Generic, and Extractive summarization proved to be the most common approaches employed, while techniques based on Machine Learning were performed in 16 studies and Rouge (Recall-Oriented Understudy for Gisting Evaluation) was reported as the evaluation metric in 26 studies. This review found that in recent years, more transformer-based methodologies for summarization purposes have been implemented compared to a previous survey. Additionally, there are still some challenges in text summarization in different domains, especially in the biomedical field in terms of demand for further research.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {intrinsic evaluation,language processing,medical documents,text summarization},
  file = {/home/glory/Zotero/storage/2K2P3CNE/Chaves et al. - 2022 - Automatic Text Summarization of Biomedical Text Data A Systematic Review.pdf}
}

@misc{nguyenComparativeStudyQuality2024,
  title = {A {{Comparative Study}} of {{Quality Evaluation Methods}} for {{Text Summarization}}},
  author = {Nguyen, Huyen and Chen, Haihua and Pobbathi, Lavanya and Ding, Junhua},
  year = {2024},
  month = jun,
  number = {arXiv:2407.00747},
  eprint = {2407.00747},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.00747},
  urldate = {2025-08-04},
  abstract = {Evaluating text summarization has been a challenging task in natural language processing (NLP). Automatic metrics which heavily rely on reference summaries are not suitable in many situations, while human evaluation is time-consuming and labor-intensive. To bridge this gap, this paper proposes a novel method based on large language models (LLMs) for evaluating text summarization. We also conducts a comparative study on eight automatic metrics, human evaluation, and our proposed LLM-based method. Seven different types of state-of-the-art (SOTA) summarization models were evaluated. We perform extensive experiments and analysis on datasets with patent documents. Our results show that LLMs evaluation aligns closely with human evaluation, while widely-used automatic metrics such as ROUGE-2, BERTScore, and SummaC do not and also lack consistency. Based on the empirical comparison, we propose a LLM-powered framework for automatically evaluating and improving text summarization, which is beneficial and could attract wide attention among the community.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/glory/Zotero/storage/S7JCGI3Q/Nguyen et al. - 2024 - A Comparative Study of Quality Evaluation Methods for Text Summarization.pdf;/home/glory/Zotero/storage/YVLDZ2M2/2407.html}
}

@article{rochaAutomatingEvaluatingLarge2025,
  title = {Automating and {{Evaluating Large Language Models}} for {{Accurate Text Summarization Under Zero-Shot Conditions}}},
  author = {Rocha, Maria Priebe Mendes and Klasky, Hilda B.},
  year = {2025},
  month = jun,
  journal = {AMIA Summits on Translational Science Proceedings},
  volume = {2025},
  pages = {461--470},
  issn = {2153-4063},
  urldate = {2025-08-04},
  abstract = {Automated text summarization (ATS) is crucial for collecting specialized, domain-specific information. Zero-shot learning (ZSL) allows large language models (LLMs) to respond to prompts on information not included in their training, playing a vital role in this process. This study evaluates LLMs' effectiveness in generating accurate summaries under ZSL conditions and explores using retrieval augmented generation (RAG) and prompt engineering to enhance factual accuracy and understanding. We combined LLMs with summarization modeling, prompt engineering, and RAG, evaluating the summaries using the METEOR metric and keyword frequencies through word clouds. Results indicate that LLMs are generally well-suited for ATS tasks, demonstrating an ability to handle specialized information under ZSL conditions with RAG. However, web scraping limitations hinder a single generalized retrieval mechanism. While LLMs show promise for ATS under ZSL conditions with RAG, challenges like goal misgeneralization and web scraping limitations need addressing. Future research should focus on solutions to these issues.},
  pmcid = {PMC12150706},
  pmid = {40502264},
  file = {/home/glory/Zotero/storage/JALVJKL5/Rocha and Klasky - 2025 - Automating and Evaluating Large Language Models for Accurate Text Summarization Under Zero-Shot Cond.pdf}
}

@article{saxenaCOMPARATIVEANALYSISLARGE2025,
  title = {{{COMPARATIVE ANALYSIS OF LARGE LANGUAGE MODELS FOR THE APPLICATION OF SCIENTIFIC ARTICLE SUMMARIZATION}}},
  author = {Saxena, Rishabh and Singh, Shubhangi and Dubey, Preeti},
  year = {2025},
  month = jan,
  journal = {Procedia Computer Science},
  series = {Sixth {{International Conference}} on {{Futuristic Trends}} in {{Networks}} and {{Computing Technologies}} ({{FTNCT06}}), Held in {{Uttarakhand}}, {{India}}},
  volume = {259},
  pages = {532--542},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2025.04.002},
  urldate = {2025-08-04},
  abstract = {Automatic text summarization has become an essential tool for academics to stay up to date with the newest advancements due to the high rise of scientific publications. Abstractive text summarisation tasks can be done using LLMs. However current summarization methods often struggle to capture the nuanced and technical details present in research papers and there are prevalent research gaps in the realm of abstractive summarisation using generative AI. This study aims to analyse the efficiency of LLM models -- GPT-3.5, LLaMa 3, Mixtral 8x7b and Gemma-2, in condensing detailed scientific literature into manageable summaries, and their further evaluation based on ROUGE, BERT and BLEU scores. The results show that GPT-3.5 and LLaMa 3 generate more coherent and contextually accurate summaries with a BERT score of 0.21 and 0.19 respectively, even though Mixtral 8x7B performs exceptionally well in quantitative measurements. Despite its coherence, Gemma-2 receives poorer performance in both qualitative and quantitative assessments. These findings bring out the value of integrating quantitative and qualitative evaluations to gain a thorough understanding of summarization.},
  keywords = {Automatic text summarization,BERT,BLEU,Gemma,GPT-3,LLaMa-3,LLMs,Mixtral,ROUGE},
  file = {/home/glory/Zotero/storage/G8XQVJWH/Saxena et al. - 2025 - COMPARATIVE ANALYSIS OF LARGE LANGUAGE MODELS FOR THE APPLICATION OF SCIENTIFIC ARTICLE SUMMARIZATIO.pdf;/home/glory/Zotero/storage/BNR6SVDG/S1877050925010993.html}
}

@article{BRIN1998107,
title = {The anatomy of a large-scale hypertextual Web search engine},
journal = {Computer Networks and ISDN Systems},
volume = {30},
number = {1},
pages = {107-117},
year = {1998},
note = {Proceedings of the Seventh International World Wide Web Conference},
issn = {0169-7552},
doi = {https://doi.org/10.1016/S0169-7552(98)00110-X},
url = {https://www.sciencedirect.com/science/article/pii/S016975529800110X},
author = {Sergey Brin and Lawrence Page},
keywords = {World Wide Web, Search engines, Information retrieval, PageRank, Google},
abstract = {In this paper, we present Google, a prototype of a large-scale search engine which makes heavy use of the structure present in hypertext. Google is designed to crawl and index the Web efficiently and produce much more satisfying search results than existing systems. The prototype with a full text and hyperlink database of at least 24 million pages is available at http://google.stanford.edu/ To engineer a search engine is a challenging task. Search engines index tens to hundreds of millions of Web pages involving a comparable number of distinct terms. They answer tens of millions of queries every day. Despite the importance of large-scale search engines on the Web, very little academic research has been done on them. Furthermore, due to rapid advance in technology and Web proliferation, creating a Web search engine today is very different from three years ago. This paper provides an in-depth description of our large-scale Web search engine â€” the first such detailed public description we know of to date. Apart from the problems of scaling traditional search techniques to data of this magnitude, there are new technical challenges involved with using the additional information present in hypertext to produce better search results. This paper addresses this question of how to build a practical large-scale system which can exploit the additional information present in hypertext. Also we look at the problem of how to effectively deal with uncontrolled hypertext collections where anyone can publish anything they want.}
}
