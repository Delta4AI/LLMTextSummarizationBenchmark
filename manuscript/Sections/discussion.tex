\section{Discussion}

Our benchmarking analysis of 62 different text summarization models on a dataset of 1,000 abstracts revealed clear performance differences. General-purpose \acp{LLM} achieved the highest summarization quality across all metric dimensions closely followed by general-purpose \acp{SLM} and reasoning-oriented \acp{LLM}. In contrast, domain-specific models, encoder–decoder architectures, and traditional extractive methods performed significantly regarding overall performance. These results highlight the clear progression from extractive and encoder–decoder approaches toward transformer-based models, while also showing that domain-specific fine-tuning alone does not necessarily lead to improved summarization quality.

Comparing models by architecture, size, and domain focus shows that, overall, \acp{LLM} perform best, likely due to their high number of parameters which enable them to better understand the complex context typical for biomedical literature. While very small models like Gemma3:270M can indeed lack the capacity to handle this complexity, \acp{SLM} remain competitive, with some models, such as mistral:7b and gpt-4o-mini, even outperforming certain \acp{LLM} across the three metric dimensions. This may be attributed to the fact that smaller datasets are often more curated and of higher quality compared to the large amount of data required to train a large model \cite{xu2025evaluatingsmalllanguagemodels}. 

Interestingly, medium-sized models (e.g many models in the Mistral family), appear to be more performant than larger proprietary ones. These models seem to reach an optimal compromise on number of parameters and overall performance where additional parameters could disrupt this equilibrium, leading to potentially over‑fitting or plateauing performance \cite{muennighoff2025scalingdataconstrainedlanguagemodels}.

Another interesting result of our analysis was that overall general-purpose models outperform both domain-specific models specialized in the biomedical domain and the one specialized for text summarization, regardless of model size. A possible explanation for this behavior might be that domain-specific models fine-tuned on biomedical text might be better for learning and understanding the complex biomedical terminology or lexical patterns but fail in summarization tasks. On the other hand, models specifically designed for text summarization might be good at summarizing in general but fail at capturing the complex biomedical meaning. That is why generalist models, leveraging their broad knowledge, seem to perform better \cite{dorfner2024biomedicallargelanguagesmodels}. Additionally, domain-specific models can “forget” the general knowledge that was acquired during the pre-training phase, experiencing a phenomenon called “catastrophic forgetting”, which represents an issue when the task requires both domain-specific knowledge, biomedical knowledge, and context understanding for text summarization \cite{zhou2025internal}.

Most reasoning-oriented models ranked in the middle, indicating moderate performance. This can be explained by the intrinsic multi-step logical reasoning nature of these models. While it can be advantageous for tasks that require breaking problems down into sequential steps like for mathematical operations or computer programming, it may not be ideal for text summarization which requires semantic compression and factual grounding instead \cite{jin2025reasoningnotcomprehensiveevaluation}.

\subsection{Limitations of the study}

Even though our results are based on a robust evaluation framework, there are several factors worth discussing. Model access methods varied across the evaluation due to differing \ac{API} capabilities and requirements. HuggingFace models were accessed through their supported interfaces: the pipeline \ac{API} (task="summarization") where available, or chat/completion formats for models that did not support the pipeline approach. Ollama models required use of the generate endpoint with merged prompts, while OpenAI, Anthropic, and Mistral models each mandated their respective provider-specific \acp{API} (responses.create, messages.create, and chat.complete) with distinct message structures. We applied hyperparameter normalization where possible, though \ac{API}-level constraints prevented full standardization. For example, GPT-5 does not support temperature control, instead offering only reasoning-specific parameters. Additionally, proprietary middleware layers may transform requests and responses in undocumented ways, potentially affecting outputs independently of the underlying model architectures. These necessary methodological variations warrant consideration when interpreting performance differences across models.

A limitation of this benchmarking study is that we focused on a single summarization task: generating concise summaries from biomedical abstracts. This setup provides a clear and well-defined evaluation framework, but the findings may not fully extend to other forms of scientific or biomedical summarization, including full-length articles, clinical trial data, or lay-oriented summaries. Given the rapid evolution of \acp{LLM}, these results just capture a specific snapshot in time and may change as newer architectures and models become available.

Another limitation lies in the exclusive reliance on automatic evaluation metrics. Although combining lexical-based, semantic-based, and factual consistency measures offers a broad view, human assessment could provide a more nuanced understanding of readability, coherence, and factual correctness. Future work could therefore extend this benchmark by integrating expert-based evaluations, exploring alternative summarization tasks, and including emerging model families as they are released.

The results of this benchmark provide useful guidance for selecting summarization models in biomedical and scientific settings. The strong performance of general-purpose \acp{LM} indicates that broad, diverse pretraining is often more advantageous than narrow domain adaptation when dealing with unseen scientific content.

Another key consideration is the trade-off between output quality and processing efficiency. While \acp{LLM} achieved the highest overall scores, \acp{SLM} deliver competitive results at substantially lower computational cost \cite{irugalbandara2024scalingscaleupcostbenefit}, which makes them especially attractive for large-scale or resource-constrained applications. Choosing between large and small models therefore depends not only on desired output quality but also on the intended scale of summarization.

Overall, the findings suggest that general-purpose \acp{LM} currently offer the most reliable and practical choice for biomedical text summarization. Their consistent performance across evaluation criteria demonstrates that broad generalization outweighs the marginal gains from more narrowly specialized or fine-tuned approaches, many of which are not primarily optimized for text summarization.