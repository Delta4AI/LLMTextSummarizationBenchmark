\section{Materials and Methods}

Figure~\ref{fig:workflow_graphic} provides an overview of the complete benchmarking workflow of this study from dataset construction and model inference to metric computation, score aggregation, and statistical analysis.

\begin{figure*}[t]
\centering
\includegraphics[width=0.9\textwidth]{Visualizations/workflow_graphic.png}
\captionsetup{skip=6pt}
\caption{
\textbf{Overview of the benchmarking workflow:}
(1) Construction of a gold-standard dataset comprising 1,000 biomedical abstracts paired with author-provided highlights sections.
(2) Suite of 62 diverse summarization models.
(3) Uniform prompting of all models to generate summaries for each abstract.
(4--5) Computation of summary quality metrics capturing lexical overlap, semantic similarity, and factual consistency.
(6) Aggregation of metrics using group-specific weights to obtain overall performance scores.
(7) Ranking of models based on overall performance scores.
\label{fig:workflow_graphic}
}
\end{figure*}

\subsection{Gold-Standard Dataset}

We generated a gold-standard benchmarking dataset comprising 1,000 biomedical peer-reviewed articles from \textit{ScienceDirect} and \textit{Cell Press} - as these publishers provide standardized highlights sections for publications \cite{ElsevierHighlights, CellPressHighlights}. These highlights sections provide concise bullet points capturing the main findings of scientific articles. The concatenated highlights served as reference summaries in our evaluation, while the corresponding abstracts were used as input texts for the summarization task. While summarization is inherently subjective, author-generated highlights represent the most credible and standardized source, ensuring the captured content reflects the study's intended key messages.

Articles were collected systematically across a variety of journals from the two publishers to ensure coverage of different fields within molecular sciences, including among others drug discovery, genomics, proteomics, biotechnology, and biochemistry. We selected 50 articles from each of 20 different journals across the two publishers, resulting in 1,000 papers as shown in Table~\ref{tab:goldstandard}.

\begin{table*}[t]
\centering
\caption{Overview of journals included in the gold-standard dataset. 
Each journal contributed 50 articles, resulting in 500 articles from \textit{ScienceDirect} and 500 articles from \textit{Cell Press}. \label{tab:goldstandard}}
\begin{tabularx}{\linewidth}{lX}
\toprule
\textbf{Publisher} & \textbf{Journals} \\
\midrule
\textit{ScienceDirect} & 
Drug Discovery Today; Journal of Molecular Biology; FEBS Letters; 
Journal of Biotechnology; Gene; Genomics; Journal of Proteomics; 
The International Journal of Biochemistry \& Cell Biology; Cytokine; 
Developmental Cell \\
\addlinespace[0.3em]
\textit{Cell Press} & 
Cell; Cancer Cell; Cell Chemical Biology; Cell Genomics; Cell Host \& Microbe; 
Cell Metabolism; Cell Reports; Cell Reports Medicine; Cell Stem Cell; 
Cell Systems \\
\bottomrule
\end{tabularx}
\end{table*}

This setup provided standardized pairs of abstracts and reference summaries subsequently used in our systematic evaluation and benchmarking of \ac{ATS} methods.

\subsection{Summarization Methods}

We in total evaluated 62 summarization models, ranging from simple frequency-based algorithms to \acp{SLM} \& \acp{LLM}. These models were classified into five categories, as listed in Table~\ref{tab:models}. We obtained the pre-trained \acp{EDM} through the Hugging Face library, selecting architectures widely used for abstractive summarization tasks to represent well established neural approaches within our benchmark. Additionally, we evaluated a range of popular \acp{SLM} and \acp{LLM}, defining \acp{SLM} as models with fewer than 10 billion parameters \cite{belcak2025smalllanguagemodelsfuture}. Both \acp{SLM} and \acp{LLM} with advanced reasoning capabilites were categorized as "reasoning-oriented" to investigate how multi-step problem solving affects summarization performance. Similarly, models fine-tuned on scientific/biomedical data or specifically tailored for text summarization were classified as "domain-specific" to assess the impact of domain adaptation. Overall, this selection covers various model sizes and release dates, ensuring a representative mix of both, widely adopted and recent, architectures.

Exceptionally large models, such as LLaMA 3.1 405B, were excluded as their computational requirements exceed those of practical summarization pipelines. Each of the 62 models was tasked with generating summaries for the 1,000 abstracts in the dataset, resulting in a total of 62,000 generated summaries for evaluation.

\begin{table*}[t]
\centering
\caption{Overview of summarization methods/models evaluated in this study, organized by category.\label{tab:models}}
\begin{tabularx}{\linewidth}{lX}
\toprule
\textbf{Category} & \textbf{Methods/Models} \\
\midrule
Traditional models & textrank; frequency \\
\midrule
General-purpose \acp{EDM} & facebook/bart-base; google-t5/t5-base; google-t5/t5-large; google/pegasus-large \\
\midrule
Domain-specific \acp{EDM} & facebook/bart-large-cnn; google/pegasusxsum; google/pegasus-cnn\_dailymail; google/pegasus-pubmed; google/bigbird-pegasuslarge-pubmed; csebuetnlp/mT5\_multilingual\_XLSum; led\_large\_16384\_arxiv\_summarization \\
\midrule
General-purpose \acp{SLM} & gemma3:270M; gemma3:1b; gemma3:4b; PetrosStav/gemma3-tools:4b; granite3.3:2b; granite3.3:8b; granite4:tiny-h; granite4:small-h; granite4:micro; granite4:micro-h; llama3.1:8b; llama3.2:1b; llama3.2:3b; mistral:7b; phi3:3.8b; gpt-4o-mini; gpt-4.1-mini; chat\_swiss-ai/Apertus-8B-Instruct-2509 \\
\midrule
General-purpose \acp{LLM} & gemma3:12b; mistral-nemo:12b; mistral-small3.2:24b; mistral-small-2506; mistral-medium-2505; mistral-large-2411; phi4:14b; gpt-3.5-turbo; gpt-4o; gpt-4.1; claude-3-5-haiku-20241022 \\
\midrule
Reasoning-oriented \acp{SLM} & deepseek-r1:1.5b; deepseek-r1:7b; deepseek-r1:8b; qwen3:4b; qwen3:8b; \\
\midrule
Reasoning-oriented \acp{LLM} & deepseek-r1:14b; gpt-oss:20b; gpt-5-nano-2025-08-07; gpt-5-mini-2025-08-07; gpt-5-2025-08-07; claude-sonnet-4-20250514; claude-opus-4-20250514; claude-opus-4-1-20250805; magistral-medium-2509 \\
\midrule
Domain-specific \acp{SLM} & completion\_microsoft/biogpt; medllama2:7b; chat\_aaditya/OpenBioLLM-Llama3-8B; conversational\_BioMistral/BioMistral-7B; chat\_Uni-SMART/SciLitLLM1.5-7B \\
\midrule
Domain-specific \acp{LLM} & chat\_Uni-SMART/SciLitLLM1.5-14B \\
\bottomrule
\end{tabularx}
\end{table*}

\subsection{Prompt Design}

To ensure comparability across models, we prompted all summarization systems with an identical task description. The prompt instructed the models to generate concise summaries focused on the main findings of each publication while excluding unnecessary background or methodological details. Each model received the publication title and abstract as an input and was asked to produce an output of 15–100 words. If the abstract did not contain any substantive results or conclusions, the model was instructed to return the predefined token INSUFFICIENT\_FINDINGS.

The exact prompt used for all models was as follows:

\begin{nolinenumbers}
\begin{Verbatim}[
  breaklines=true,
  breakanywhere=true,
  breaksymbolleft={},
  breaksymbolright={}
]
Summarize the provided publication (title and abstract) in 15-100 words.
Key requirements:
- Identify main findings, results, or contributions
- Preserve essential context and nuance
- Exclude background, methods unless crucial to conclusions
- Write concisely and objectively
- Avoid repetition and unnecessary qualifiers
If no substantial findings exist, respond: 'INSUFFICIENT_FINDINGS'
\end{Verbatim}
\end{nolinenumbers}

\subsection{Evaluation Metrics}

As there is no single metric that can fully reflect summary quality, especially in the biomedical field where both coverage of key information and factual correctness are critical, we employed both surface-level metrics based on lexical overlap, referred to as lexical-based metrics, and embedding-level metrics. The latter includes metrics based on semantic similarity, denoted as semantic-based metrics, as well as one metric that evaluates factual consistency.

\subsubsection{Surface-level Metrics}

Surface-level metrics compare the generated summaries with the reference summaries mainly at the word or phrase level. While they do not capture meaning beyond surface overlap, they remain common metrics in summarization research and provide a straightforward foundation for evaluation. We used three \ac{ROUGE} variants (ROUGE-1, ROUGE-2, ROUGE-L) \cite{lin-2004-rouge}, \ac{BLEU} \cite{papineni-etal-2002-bleu}, and \ac{METEOR} \cite{banerjee-lavie-2005-meteor}. ROUGE-1 and ROUGE-2 measure how many unigrams (single words) or bigrams (word pairs) from the reference appear in the generated output, while ROUGE-L identifies the longest sequence of words shared between the two. \ac{BLEU} calculates how many n-grams in the output also occur in the reference, emphasizing precision over recall and applying a brevity penalty to counteract the tendency toward overly short summaries. \ac{METEOR} extends n-gram matching by considering word stems and synonyms, making it more robust to wording variations. Together, these metrics offer a simple but transparent point of reference.

\subsubsection{Embedding-based Metrics}

To capture similarity beyond surface-level word overlap, we included a set of embedding-based metrics built on pre-trained transformer models. These methods generate vector representations of text, allowing them to capture semantic similarity rather than just word overlap. We employed \ac{RoBERTa} \cite{liu2019robertarobustlyoptimizedbert} and \ac{DeBERTa} \cite{he2021debertadecodingenhancedbertdisentangled}, two transformer-based models with strong performance across \ac{NLP} tasks. In summarization evaluation, they can assess whether two summaries capture the same content even if phrased differently.

We further included all-mpnet-base-v2 \cite{song2020mpnetmaskedpermutedpretraining}, a transformer model fine-tuned for sentence similarity. Unlike \ac{RoBERTa} and \ac{DeBERTa}, which are general-purpose encoders, \ac{MPNet} was trained with a focus on alignment at the sentence-level. This characteristic makes it a useful complement to the other metrics, as it is particularly sensitive to whether the overall meaning of a reference summary is preserved in the system output.

Finally, to evaluate factual consistency, we applied AlignScore \cite{zha2023alignscoreevaluatingfactualconsistency}, a metric designed to assess whether the statements in a generated summary are supported by the source text. In contrast to the other metrics, AlignScore compares the output to the input text itself (i.e. the publication abstract) rather than the reference summary (i.e. the highlights section), as factual accuracy can only be assessed relative to the original input text. This addition ensures that our evaluation captures errors and hallucinations that might otherwise be overlooked.

\subsubsection{Overall Performance Metric}
\label{subsec:overall_performance_metric}

To comprehensively assess the performance of each model on the summarization task, we employed a multi-metric framework covering three dimensions: lexical (n=5), semantic (n=3), and factual (n=1). To prevent the impact of dimension imbalance, we first computed an average score for each category as follows: 

{\scriptsize
\begin{equation}
\mathrm{avg}_{\text{lex}} =
\frac{
\mathrm{ROUGE\text{-}1} + \mathrm{ROUGE\text{-}2} + \mathrm{ROUGE\text{-}L}+ \mathrm{METEOR} + \mathrm{BLEU}
}{5}
\end{equation}
}

{\scriptsize
\begin{equation}
\mathrm{avg}_{\text{sem}} =
\frac{
\mathrm{RoBERTa} + \mathrm{DeBERTa} + \mathrm{all\text{-}mpnet\text{-}base\text{-}v2}
}{3}
\end{equation}
}

{\scriptsize
\begin{equation}
\mathrm{avg}_{\text{fact}} = \mathrm{AlignScore}
\end{equation}
}

To examine how the different evaluation metrics relate to each other, we computed pairwise Spearman correlation coefficients across all models (Figure~\ref{fig:metric_correlation}). Averaging metrics within each dimension ensures that highly correlated measures such as ROUGE-1, ROUGE-2, and ROUGE-L do not dominate the overall evaluation. Averaged scores were used to construct the overall performance score, weighting each dimension according to literature-based evidences. Semantic metrics for example correlate more strongly with human judgment than lexical ones and thus were given a slightly higher overall weight \cite{fabbri2021summevalreevaluatingsummarizationevaluation}. Furthermore, we accounted for the fact that AlignScore, due to its different point of reference, tends to favor extractive approaches \cite{durmus-etal-2020-feqa}, while showing only a moderate correlation with both lexical and semantic metrics (Figure~\ref{fig:metric_correlation}). Based on these considerations, we defined an Overall Performance Score (OPS) as a weighted combination of the three dimension-wise average scores:

{\scriptsize
\begin{equation}
\mathrm{OPS}
= 0.35 \times \mathrm{avg}_{\text{lex}}
+ 0.40 \times \mathrm{avg}_{\text{sem}}
+ 0.25 \times \mathrm{avg}_{\text{fact}}
\end{equation}
}

We finally ranked models based on their overall performance scores, constructing the performance rank, with lower ranks indicating better performance. To examine which model performed best for each dimension (lexical, semantic, factual), we also ranked models based on average scores for each dimension.

\subsection{Statistical Analysis}

To assess differences in overall performance between model categories and families, we first applied Welch’s \ac{ANOVA} and conducted pairwise post-hoc comparisons using the Games-Howell test. This procedure is well suited for groups with unequal sample sizes and heterogeneous variances, conditions that apply to our benchmark due to the heterogeneity of models within each category and family. Adjusted p-values were used to determine the statistical significance of between-group differences.

\subsection{Benchmarking Framework}

The benchmark was conducted using Python 3.12. Gold standard data were retrieved from open-access articles published by \textit{ScienceDirect} and \textit{Cell Press} through manual extraction of titles, abstracts, and highlights sections, along with metadata including publication URLs, identifiers, section types, and article types where available. All data were stored in machine-readable JSON format.

The framework was implemented using the Python standard library supplemented by several specialized packages: pandas \cite{mckinney-proc-scipy-2010} for data import and export, scikit-learn \cite{scikit-learn} for computing cosine similarities of embeddings and TF-IDF vectors, networkx \cite{hagberg2008exploring} for graph construction and PageRank algorithm \cite{BRIN1998107}. Additional evaluation metrics were computed using NLTK \cite{bird2009natural} for METEOR and BLEU scores, ROUGE-score, BERT-score \cite{zhang2020bertscoreevaluatingtextgeneration}, AlignScore, and sentence-transformers \cite{reimers2019sentencebertsentenceembeddingsusing} with the all-mpnet-base-v2 model.

Communication with proprietary closed-source LLMs was facilitated through the official Python APIs provided by Anthropic, Mistral AI, and OpenAI. Local LLM execution was performed on a workstation equipped with a NVIDIA RTX A4000 GPU (16GB VRAM) running Ollama as a backend service, accessed through its Python API along with the transformers library \cite{wolf-etal-2020-transformers}. 

All LLMs were configured with a temperature parameter of 0.2 to optimize reproducibility while avoiding completely deterministic outputs. For the latest generation of OpenAI models featuring adaptive reasoning capabilities, the configuration was set to \texttt{text.verbosity = low} and \texttt{reasoning.effort = minimal}. The full set of parameters and prompts are documented in the \texttt{config.py} file in the GitHub repository.

\subsection{Data Availability}

The complete source code, documentation, gold standard dataset, and processed results are available at:

\url{https://www.github.com/Delta4AI/LLMTextSummarizationBenchmark}.
