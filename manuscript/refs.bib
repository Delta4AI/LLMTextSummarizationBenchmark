@misc{zhang2025comprehensivesurveyprocessorientedautomatic,
  title={A Comprehensive Survey on Process-Oriented Automatic Text Summarization with Exploration of LLM-Based Methods}, 
  author={Yang Zhang and Hanlei Jin and Dan Meng and Jun Wang and Jinghua Tan},
  year={2025},
  eprint={2403.02901},
  archivePrefix={arXiv},
  primaryClass={cs.AI},
  url={https://arxiv.org/abs/2403.02901}, 
}

@misc{zhang2024systematicsurveytextsummarization,
  title={A Systematic Survey of Text Summarization: From Statistical Methods to Large Language Models}, 
  author={Haopeng Zhang and Philip S. Yu and Jiawei Zhang},
  year={2024},
  eprint={2406.11289},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2406.11289}, 
}

@article{ROHIL2022100058,
  title = {An exploratory study of automatic text summarization in biomedical and healthcare domain},
  journal = {Healthcare Analytics},
  volume = {2},
  pages = {100058},
  year = {2022},
  issn = {2772-4425},
  doi = {https://doi.org/10.1016/j.health.2022.100058},
  url = {https://www.sciencedirect.com/science/article/pii/S2772442522000223},
  author = {Mukesh Kumar Rohil and Varun Magotra},
  keywords = {Artificial Intelligence, Electronic Health Records, Automatic Text Summarization, Natural Language Processing, Unified Medical Language System, Medical Subject Headings},
  abstract = {In the last two decades, the uses of automatic text summarization have been realized in a wide range of applications in various fields cutting across a number of verticals. Amongst these, one of the most inquired is the domain of healthcare and medicine. Many of the studies have revealed that the use of automatic text summarization in the biomedical and healthcare domain helps researchers and medical professionals save their time and access more information in considerably short spans of time. This article reports some of the recent studies that enumerate the benefits and limitations of the uses of automatic text summarization in the biomedical and healthcare domain. In addition, the paper also explores certain new possible applications of automatic text summarization in the biomedical and healthcare domain. Furthermore, it discusses the trends and vision towards future opportunities for possible research in automatic text summarization in the context of medical and healthcare domain.}
}

@misc{xie2023surveybiomedicaltextsummarization,
  title={A Survey for Biomedical Text Summarization: From Pre-trained to Large Language Models}, 
  author={Qianqian Xie and Zheheng Luo and Benyou Wang and Sophia Ananiadou},
  year={2023},
  eprint={2304.08763},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2304.08763}, 
}

@article{Luhn1958TheAC,
  title={The Automatic Creation of Literature Abstracts},
  author={Hans Peter Luhn},
  journal={IBM J. Res. Dev.},
  year={1958},
  volume={2},
  pages={159-165},
  url={https://api.semanticscholar.org/CorpusID:15475171}
}

@article{10.1145/321510.321519,
  author = {Edmundson, H. P.},
  title = {New Methods in Automatic Extracting},
  year = {1969},
  issue_date = {April 1969},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {16},
  number = {2},
  issn = {0004-5411},
  url = {https://doi.org/10.1145/321510.321519},
  doi = {10.1145/321510.321519},
  abstract = {This paper describes new methods of automatically extracting documents for screening purposes, i.e. the computer selection of sentences having the greatest potential for conveying to the reader the substance of the document. While previous work has focused on one component of sentence significance, namely, the presence of high-frequency content words (key words), the methods described here also treat three additional components: pragmatic words (cue words); title and heading words; and structural indicators (sentence location).The research has resulted in an operating system and a research methodology. The extracting system is parameterized to control and vary the influence of the above four components. The research methodology includes procedures for the compilation of the required dictionaries, the setting of the control parameters, and the comparative evaluation of the automatic extracts with manually produced extracts. The results indicate that the three newly proposed components dominate the frequency component in the production of better extracts.},
  journal = {J. ACM},
  month = apr,
  pages = {264–285},
  numpages = {22}
}

@article{article,
  author = {Robertson, Stephen},
  year = {2004},
  month = {10},
  pages = {503-520},
  title = {Understanding Inverse Document Frequency: On Theoretical Arguments for IDF},
  volume = {60},
  journal = {Journal of Documentation - J DOC},
  doi = {10.1108/00220410410560582}
}

@inproceedings{10.1145/1183614.1183701,
  author = {Reeve, Lawrence H. and Han, Hyoil and Nagori, Saya V. and Yang, Jonathan C. and Schwimmer, Tamara A. and Brooks, Ari D.},
  title = {Concept frequency distribution in biomedical text summarization},
  year = {2006},
  isbn = {1595934332},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/1183614.1183701},
  doi = {10.1145/1183614.1183701},
  abstract = {Text summarization is a data reduction process. The use of text summarization enables users to reduce the amount of text that must be read while still assimilating the core information. The data reduction offered by text summarization is particularly useful in the biomedical domain, where physicians must continuously find clinical trial study information to incorporate into their patient treatment efforts. Such efforts are often hampered by the high-volume of publications. Our contribution is two-fold: 1) to propose the frequency of domain concepts as a method to identify important sentences within a full-text; and 2) propose a novel frequency distribution model and algorithm for identifying important sentences based on term or concept frequency distribution. An evaluation of several existing summarization systems using biomedical texts is presented in order to determine a performance baseline. For domain concept comparison, a recent high-performing frequency-based algorithm using terms is adapted to use concepts and evaluated using both terms and concepts. It is shown that the use of concepts performs closely with the use of terms for sentence selection. Our proposed frequency distribution model and algorithm outperforms a state-of-the-art approach.},
  booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
  pages = {604–611},
  numpages = {8},
  keywords = {text summarization, concept frequency, biomedicine},
  location = {Arlington, Virginia, USA},
  series = {CIKM '06}
}

@inproceedings{mihalcea-tarau-2004-textrank,
  title = "{T}ext{R}ank: Bringing Order into Text",
  author = "Mihalcea, Rada  and
    Tarau, Paul",
  editor = "Lin, Dekang  and
    Wu, Dekai",
  booktitle = "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing",
  month = jul,
  year = "2004",
  address = "Barcelona, Spain",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/W04-3252/",
  pages = "404--411"
}

@Article{afzal2020,
  author="Afzal, Muhammad
  and Alam, Fakhare
  and Malik, Khalid Mahmood
  and Malik, Ghaus M",
  title="Clinical Context--Aware Biomedical Text Summarization Using Deep Neural Network: Model Development and Validation",
  journal="J Med Internet Res",
  year="2020",
  month="Oct",
  day="23",
  volume="22",
  number="10",
  pages="e19810",
  keywords="biomedical informatics; automatic text summarization; deep neural network; word embedding; semantic similarity; brain aneurysm",
  abstract="Background: Automatic text summarization (ATS) enables users to retrieve meaningful evidence from big data of biomedical repositories to make complex clinical decisions. Deep neural and recurrent networks outperform traditional machine-learning techniques in areas of natural language processing and computer vision; however, they are yet to be explored in the ATS domain, particularly for medical text summarization. Objective: Traditional approaches in ATS for biomedical text suffer from fundamental issues such as an inability to capture clinical context, quality of evidence, and purpose-driven selection of passages for the summary. We aimed to circumvent these limitations through achieving precise, succinct, and coherent information extraction from credible published biomedical resources, and to construct a simplified summary containing the most informative content that can offer a review particular to clinical needs. Methods: In our proposed approach, we introduce a novel framework, termed Biomed-Summarizer, that provides quality-aware Patient/Problem, Intervention, Comparison, and Outcome (PICO)-based intelligent and context-enabled summarization of biomedical text. Biomed-Summarizer integrates the prognosis quality recognition model with a clinical context--aware model to locate text sequences in the body of a biomedical article for use in the final summary. First, we developed a deep neural network binary classifier for quality recognition to acquire scientifically sound studies and filter out others. Second, we developed a bidirectional long-short term memory recurrent neural network as a clinical context--aware classifier, which was trained on semantically enriched features generated using a word-embedding tokenizer for identification of meaningful sentences representing PICO text sequences. Third, we calculated the similarity between query and PICO text sequences using Jaccard similarity with semantic enrichments, where the semantic enrichments are obtained using medical ontologies. Last, we generated a representative summary from the high-scoring PICO sequences aggregated by study type, publication credibility, and freshness score. Results: Evaluation of the prognosis quality recognition model using a large dataset of biomedical literature related to intracranial aneurysm showed an accuracy of 95.41{\%} (2562/2686) in terms of recognizing quality articles. The clinical context--aware multiclass classifier outperformed the traditional machine-learning algorithms, including support vector machine, gradient boosted tree, linear regression, K-nearest neighbor, and na{\"i}ve Bayes, by achieving 93{\%} (16127/17341) accuracy for classifying five categories: aim, population, intervention, results, and outcome. The semantic similarity algorithm achieved a significant Pearson correlation coefficient of 0.61 (0-1 scale) on a well-known BIOSSES dataset (with 100 pair sentences) after semantic enrichment, representing an improvement of 8.9{\%} over baseline Jaccard similarity. Finally, we found a highly positive correlation among the evaluations performed by three domain experts concerning different metrics, suggesting that the automated summarization is satisfactory. Conclusions: By employing the proposed method Biomed-Summarizer, high accuracy in ATS was achieved, enabling seamless curation of research evidence from the biomedical literature to use for clinical decision-making. ",
  issn="1438-8871",
  doi="10.2196/19810",
  url="http://www.jmir.org/2020/10/e19810/",
  url="https://doi.org/10.2196/19810",
  url="http://www.ncbi.nlm.nih.gov/pubmed/33095174"
}

@article{almasoud2022,
  author = {Almasoud, Ahmed and Hassine, Siwar and Al-Wesabi, Fahd and Nour, Mohamed and Hilal, Anwer and Al Duhayyim, Mesfer and Hamza, Ahmed and Motwakel, Abdelwahed},
  year = {2022},
  month = {01},
  pages = {5800},
  title = {Automated Multi-Document Biomedical Text Summarization Using Deep Learning Model},
  volume = {71},
  journal = {Computers, Materials \& Continua},
  doi = {10.32604/cmc.2022.024556}
}

@misc{vaswani2023attentionneed,
  title={Attention Is All You Need}, 
  author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  year={2023},
  eprint={1706.03762},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/1706.03762}, 
}

@misc{devlin2019bertpretrainingdeepbidirectional,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
  author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  year={2019},
  eprint={1810.04805},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/1810.04805}, 
}

@article{DBLP:journals/corr/abs-1910-13461,
  author    = {Mike Lewis and
               Yinhan Liu and
               Naman Goyal and
               Marjan Ghazvininejad and
               Abdelrahman Mohamed and
               Omer Levy and
               Veselin Stoyanov and
               Luke Zettlemoyer},
  title     = {{BART:} Denoising Sequence-to-Sequence Pre-training for Natural Language
               Generation, Translation, and Comprehension},
  journal   = {CoRR},
  volume    = {abs/1910.13461},
  year      = {2019},
  url       = {http://arxiv.org/abs/1910.13461},
  eprinttype = {arXiv},
  eprint    = {1910.13461},
  timestamp = {Thu, 31 Oct 2019 14:02:26 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-13461.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{yuan-etal-2022-biobart,
  title = "{B}io{BART}: Pretraining and Evaluation of A Biomedical Generative Language Model",
  author = "Yuan, Hongyi  and
    Yuan, Zheng  and
    Gan, Ruyi  and
    Zhang, Jiaxing  and
    Xie, Yutao  and
    Yu, Sheng",
  editor = "Demner-Fushman, Dina  and
    Cohen, Kevin Bretonnel  and
    Ananiadou, Sophia  and
    Tsujii, Junichi",
  booktitle = "Proceedings of the 21st Workshop on Biomedical Language Processing",
  month = may,
  year = "2022",
  address = "Dublin, Ireland",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2022.bionlp-1.9/",
  doi = "10.18653/v1/2022.bionlp-1.9",
  pages = "97--109",
  abstract = "Pretrained language models have served as important backbones for natural language processing. Recently, in-domain pretraining has been shown to benefit various domain-specific downstream tasks. In the biomedical domain, natural language generation (NLG) tasks are of critical importance, while understudied. Approaching natural language understanding (NLU) tasks as NLG achieves satisfying performance in the general domain through constrained language generation or language prompting. We emphasize the lack of in-domain generative language models and the unsystematic generative downstream benchmarks in the biomedical domain, hindering the development of the research community. In this work, we introduce the generative language model BioBART that adapts BART to the biomedical domain. We collate various biomedical language generation tasks including dialogue, summarization, entity linking, and named entity recognition. BioBART pretrained on PubMed abstracts has enhanced performance compared to BART and set strong baselines on several tasks. Furthermore, we conduct ablation studies on the pretraining tasks for BioBART and find that sentence permutation has negative effects on downstream tasks."
}

@article{abinaya2024,
  author = {Abinaya, S and Vigil, M.s.Antony and Keerthika, K and Varshasri, R},
  year = {2024},
  month = {01},
  pages = {},
  title = {Medical Text Summarization Using BART with LoRA-Based Parameter Efficient Fine Tuning}
}

@article{JMLR:v21:20-074,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {140},
  pages   = {1--67},
  url     = {http://jmlr.org/papers/v21/20-074.html}
}

@misc{zhang2020pegasuspretrainingextractedgapsentences,
  title={PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization}, 
  author={Jingqing Zhang and Yao Zhao and Mohammad Saleh and Peter J. Liu},
  year={2020},
  eprint={1912.08777},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/1912.08777}, 
}

@misc{Beltagy2020Longformer,
  title={Longformer: The Long-Document Transformer}, 
  author={Iz Beltagy and Matthew E. Peters and Arman Cohan},
  year={2020},
  eprint={2004.05150},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2004.05150}, 
}

@article{steblianko2024,
  author = {Steblianko, Oleksandr and Shymkovych, Volodymyr and Kravets, Peter and Novatskyi, Anatolii and Shymkovych, Lyubov},
  year = {2024},
  month = {12},
  pages = {150-158},
  title = {Scientific article summarization model with unbounded input length},
  journal = {Information, Computing and Intelligent systems},
  doi = {10.20535/2786-8729.5.2024.314724}
}

@misc{plaat2025multistepreasoninglargelanguage,
  title={Multi-Step Reasoning with Large Language Models, a Survey}, 
  author={Aske Plaat and Annie Wong and Suzan Verberne and Joost Broekens and Niki van Stein and Thomas Back},
  year={2025},
  eprint={2407.11511},
  archivePrefix={arXiv},
  primaryClass={cs.AI},
  url={https://arxiv.org/abs/2407.11511}, 
}

@inproceedings{Radford2018ImprovingLU,
  title={Improving Language Understanding by Generative Pre-Training},
  author={Alec Radford and Karthik Narasimhan},
  year={2018},
  url={https://api.semanticscholar.org/CorpusID:49313245}
}

@misc{bai2022constitutionalaiharmlessnessai,
  title={Constitutional AI: Harmlessness from AI Feedback}, 
  author={Yuntao Bai and Saurav Kadavath and Sandipan Kundu and Amanda Askell and Jackson Kernion and Andy Jones and Anna Chen and Anna Goldie and Azalia Mirhoseini and Cameron McKinnon and others},
  year={2022},
  eprint={2212.08073},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2212.08073}, 
}

@misc{grattafiori2024llama3herdmodels,
  title={The Llama 3 Herd of Models}, 
  author={Aaron Grattafiori and Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al-Dahle and Aiesha Letman and Akhil Mathur and Alan Schelten and Alex Vaughan and others},      year={2024},
  eprint={2407.21783},
  archivePrefix={arXiv},
  primaryClass={cs.AI},
  url={https://arxiv.org/abs/2407.21783}, 
}

@misc{OpenBioLLMs,
  author = {Ankit Pal, Malaikannan Sankarasubbu},
  title = {OpenBioLLMs: Advancing Open-Source Large Language Models for Healthcare and Life Sciences},
  year = {2024},
  publisher = {Hugging Face},
  journal = {Hugging Face repository},
  howpublished = {\url{https://huggingface.co/aaditya/OpenBioLLM-Llama3-70B}}
}

@misc{touvron2023llama2openfoundation,
  title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
  author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and others},
  year={2023},
  eprint={2307.09288},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2307.09288}, 
}

@misc{gemmateam2025gemma3technicalreport,
  title={Gemma 3 Technical Report}, 
  author={Gemma Team and Aishwarya Kamath and Johan Ferret and Shreya Pathak and Nino Vieillard and Ramona Merhej and Sarah Perrin and Tatiana Matejovicova and Alexandre Ramé and Morgane Rivière and others},
  year={2025},
  eprint={2503.19786},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2503.19786}, 
}

@misc{abdin2024phi3technicalreporthighly,
  title={Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone}, 
  author={Marah Abdin and Jyoti Aneja and Hany Awadalla and Ahmed Awadallah and Ammar Ahmad Awan and Nguyen Bach and Amit Bahree and Arash Bakhtiari and Jianmin Bao and Harkirat Behl and others},
  year={2024},
  eprint={2404.14219},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2404.14219}, 
}

@misc{abdin2024phi4technicalreport,
  title={Phi-4 Technical Report}, 
  author={Marah Abdin and Jyoti Aneja and Harkirat Behl and Sébastien Bubeck and Ronen Eldan and Suriya Gunasekar and Michael Harrison and Russell J. Hewett and Mojan Javaheripi and Piero Kauffmann and others},
  year={2024},
  eprint={2412.08905},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2412.08905}, 
}

@article{10.1093/bib/bbac409,
  author = {Luo, Renqian and Sun, Liai and Xia, Yingce and Qin, Tao and Zhang, Sheng and Poon, Hoifung and Liu, Tie-Yan},
  title = "{BioGPT: generative pre-trained transformer for biomedical text generation and mining}",
  journal = {Briefings in Bioinformatics},
  volume = {23},
  number = {6},
  year = {2022},
  month = {09},
  abstract = "{Pre-trained language models have attracted increasing attention in the biomedical domain, inspired by their great success in the general natural language domain. Among the two main branches of pre-trained language models in the general language domain, i.e. BERT (and its variants) and GPT (and its variants), the first one has been extensively studied in the biomedical domain, such as BioBERT and PubMedBERT. While they have achieved great success on a variety of discriminative downstream biomedical tasks, the lack of generation ability constrains their application scope. In this paper, we propose BioGPT, a domain-specific generative Transformer language model pre-trained on large-scale biomedical literature. We evaluate BioGPT on six biomedical natural language processing tasks and demonstrate that our model outperforms previous models on most tasks. Especially, we get 44.98\%, 38.42\% and 40.76\% F1 score on BC5CDR, KD-DTI and DDI end-to-end relation extraction tasks, respectively, and 78.2\% accuracy on PubMedQA, creating a new record. Our case study on text generation further demonstrates the advantage of BioGPT on biomedical literature to generate fluent descriptions for biomedical terms.}",
  issn = {1477-4054},
  doi = {10.1093/bib/bbac409},
  url = {https://doi.org/10.1093/bib/bbac409},
  note = {bbac409},
  eprint = {https://academic.oup.com/bib/article-pdf/23/6/bbac409/47144271/bbac409.pdf},
}

@misc{mishra2024granitecodemodelsfamily,
  title={Granite Code Models: A Family of Open Foundation Models for Code Intelligence}, 
  author={Mayank Mishra and Matt Stallone and Gaoyuan Zhang and Yikang Shen and Aditya Prasad and Adriana Meza Soria and Michele Merler and Parameswaran Selvam and Saptha Surendran and Shivdeep Singh and others},
  year={2024},
  eprint={2405.04324},
  archivePrefix={arXiv},
  primaryClass={cs.AI},
  url={https://arxiv.org/abs/2405.04324}, 
}

@misc{jiang2023mistral7b,
  title={Mistral 7B}, 
  author={Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and others},
  year={2023},
  eprint={2310.06825},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2310.06825}, 
}

@misc{mistralai2025magistral,
  title={Magistral}, 
  author={Mistral-AI and : and Abhinav Rastogi and Albert Q. Jiang and Andy Lo and Gabrielle Berrada and Guillaume Lample and Jason Rute and Joep Barmentlo and Karmesh Yadav and Kartik Khandelwal and others},
  year={2025},
  eprint={2506.10910},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2506.10910}, 
}

@misc{labrak2024biomistral,
  title={BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains}, 
  author={Yanis Labrak and Adrien Bazoge and Emmanuel Morin and Pierre-Antoine Gourraud and Mickael Rouvier and Richard Dufour},
  year={2024},
  eprint={2402.10373},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}

@misc{qwen3technicalreport,
  title={Qwen3 Technical Report}, 
  author={Qwen Team},
  year={2025},
  eprint={2505.09388},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2505.09388}, 
}

@misc{li2025scilitllmadaptllmsscientific,
  title={SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding}, 
  author={Sihang Li and Jin Huang and Jiaxi Zhuang and Yaorui Shi and Xiaochen Cai and Mingjun Xu and Xiang Wang and Linfeng Zhang and Guolin Ke and Hengxing Cai},
  year={2025},
  eprint={2408.15545},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2408.15545}, 
}

@misc{wang2025reviewdeepseekmodelskey,
  title={A Review of DeepSeek Models' Key Innovative Techniques}, 
  author={Chengen Wang and Murat Kantarcioglu},
  year={2025},
  eprint={2503.11486},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2503.11486}, 
}

@misc{swissai2025apertus,
  title={{Apertus: Democratizing Open and Compliant LLMs for Global Language Environments}},
  author={Alejandro Hernández-Cano and Alexander Hägele and Allen Hao Huang and Angelika Romanou and Antoni-Joan Solergibert and Barna Pasztor and Bettina Messmer and Dhia Garbaya and Eduard Frank Ďurech and Ido Hakimi and others},
  year={2025},
  howpublished={\url{https://arxiv.org/abs/2509.14233}}
}

@misc{ElsevierHighlights,
  author = {{Elsevier}},
  title  = {Highlights},
  year   = {2024},
  note   = {\url{https://www.elsevier.com/researcher/author/tools-and-resources/highlights} (accessed: 2025-08-07)}
}

@misc{CellPressHighlights,
  author = {{Cell Press}},
  title  = {Final Submission: Other Components: Highlights},
  year   = {2024},
  note   = {\url{https://www.cell.com/cell/information-for-authors/final-submission} (accessed: 2025-08-07)}
}

@misc{belcak2025smalllanguagemodelsfuture,
  title={Small Language Models are the Future of Agentic AI}, 
  author={Peter Belcak and Greg Heinrich and Shizhe Diao and Yonggan Fu and Xin Dong and Saurav Muralidharan and Yingyan Celine Lin and Pavlo Molchanov},
  year={2025},
  eprint={2506.02153},
  archivePrefix={arXiv},
  primaryClass={cs.AI},
  url={https://arxiv.org/abs/2506.02153}, 
}

@inproceedings{lin-2004-rouge,
  title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
  author = "Lin, Chin-Yew",
  booktitle = "Text Summarization Branches Out",
  month = jul,
  year = "2004",
  address = "Barcelona, Spain",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/W04-1013/",
  pages = "74--81"
}

@inproceedings{papineni-etal-2002-bleu,
  title = "{B}leu: a Method for Automatic Evaluation of Machine Translation",
  author = "Papineni, Kishore  and
    Roukos, Salim  and
    Ward, Todd  and
    Zhu, Wei-Jing",
  editor = "Isabelle, Pierre  and
    Charniak, Eugene  and
    Lin, Dekang",
  booktitle = "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
  month = jul,
  year = "2002",
  address = "Philadelphia, Pennsylvania, USA",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/P02-1040/",
  doi = "10.3115/1073083.1073135",
  pages = "311--318"
}

@inproceedings{banerjee-lavie-2005-meteor,
  title = "{METEOR}: An Automatic Metric for {MT} Evaluation with Improved Correlation with Human Judgments",
  author = "Banerjee, Satanjeev  and
    Lavie, Alon",
  editor = "Goldstein, Jade  and
    Lavie, Alon  and
    Lin, Chin-Yew  and
    Voss, Clare",
  booktitle = "Proceedings of the {ACL} Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization",
  month = jun,
  year = "2005",
  address = "Ann Arbor, Michigan",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/W05-0909/",
  pages = "65--72"
}

@misc{liu2019robertarobustlyoptimizedbert,
  title={RoBERTa: A Robustly Optimized BERT Pretraining Approach}, 
  author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
  year={2019},
  eprint={1907.11692},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/1907.11692}, 
}

@misc{he2021debertadecodingenhancedbertdisentangled,
  title={DeBERTa: Decoding-enhanced BERT with Disentangled Attention}, 
  author={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},
  year={2021},
  eprint={2006.03654},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2006.03654}, 
}

@misc{song2020mpnetmaskedpermutedpretraining,
  title={MPNet: Masked and Permuted Pre-training for Language Understanding}, 
  author={Kaitao Song and Xu Tan and Tao Qin and Jianfeng Lu and Tie-Yan Liu},
  year={2020},
  eprint={2004.09297},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2004.09297}, 
}

@misc{zha2023alignscoreevaluatingfactualconsistency,
  title={AlignScore: Evaluating Factual Consistency with a Unified Alignment Function}, 
  author={Yuheng Zha and Yichi Yang and Ruichen Li and Zhiting Hu},
  year={2023},
  eprint={2305.16739},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2305.16739}, 
}

@misc{fabbri2021summevalreevaluatingsummarizationevaluation,
  title={SummEval: Re-evaluating Summarization Evaluation}, 
  author={Alexander R. Fabbri and Wojciech Kryściński and Bryan McCann and Caiming Xiong and Richard Socher and Dragomir Radev},
  year={2021},
  eprint={2007.12626},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2007.12626}, 
}

@inproceedings{durmus-etal-2020-feqa,
  title = "{FEQA}: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization",
  author = "Durmus, Esin  and
    He, He  and
    Diab, Mona",
  editor = "Jurafsky, Dan  and
    Chai, Joyce  and
    Schluter, Natalie  and
    Tetreault, Joel",
  booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
  month = jul,
  year = "2020",
  address = "Online",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2020.acl-main.454/",
  doi = "10.18653/v1/2020.acl-main.454",
  pages = "5055--5070",
  abstract = "Neural abstractive summarization models are prone to generate content inconsistent with the source document, i.e. unfaithful. Existing automatic metrics do not capture such mistakes effectively. We tackle the problem of evaluating faithfulness of a generated summary given its source document. We first collected human annotations of faithfulness for outputs from numerous models on two datasets. We find that current models exhibit a trade-off between abstractiveness and faithfulness: outputs with less word overlap with the source document are more likely to be unfaithful. Next, we propose an automatic question answering (QA) based metric for faithfulness, FEQA, which leverages recent advances in reading comprehension. Given question-answer pairs generated from the summary, a QA model extracts answers from the document; non-matched answers indicate unfaithful information in the summary. Among metrics based on word overlap, embedding similarity, and learned language understanding models, our QA-based metric has significantly higher correlation with human faithfulness scores, especially on highly abstractive summaries."
}

@InProceedings{ mckinney-proc-scipy-2010,
  author    = { Wes McKinney },
  title     = { Data Structures for Statistical Computing in Python },
  booktitle = { Proceedings of the 9th Python in Science Conference },
  pages     = { 51 - 56 },
  year      = { 2010 },
  editor    = { St\'efan van der Walt and Jarrod Millman }
}

@article{scikit-learn,
  title={Scikit-learn: Machine Learning in {P}ython},
  author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
          and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
          and Weiss, R. and Dubourg, V. and others},
  journal={Journal of Machine Learning Research},
  volume={12},
  pages={2825--2830},
  year={2011}
}

@techreport{hagberg2008exploring,
  title={Exploring network structure, dynamics, and function using NetworkX},
  author={Hagberg, Aric and Swart, Pieter and S Chult, Daniel},
  year={2008},
  institution={Los Alamos National Lab.(LANL), Los Alamos, NM (United States)}
}

@article{BRIN1998107,
  title = {The anatomy of a large-scale hypertextual Web search engine},
  journal = {Computer Networks and ISDN Systems},
  volume = {30},
  number = {1},
  pages = {107-117},
  year = {1998},
  note = {Proceedings of the Seventh International World Wide Web Conference},
  issn = {0169-7552},
  doi = {https://doi.org/10.1016/S0169-7552(98)00110-X},
  url = {https://www.sciencedirect.com/science/article/pii/S016975529800110X},
  author = {Sergey Brin and Lawrence Page},
  keywords = {World Wide Web, Search engines, Information retrieval, PageRank, Google},
  abstract = {In this paper, we present Google, a prototype of a large-scale search engine which makes heavy use of the structure present in hypertext. Google is designed to crawl and index the Web efficiently and produce much more satisfying search results than existing systems. The prototype with a full text and hyperlink database of at least 24 million pages is available at http://google.stanford.edu/ To engineer a search engine is a challenging task. Search engines index tens to hundreds of millions of Web pages involving a comparable number of distinct terms. They answer tens of millions of queries every day. Despite the importance of large-scale search engines on the Web, very little academic research has been done on them. Furthermore, due to rapid advance in technology and Web proliferation, creating a Web search engine today is very different from three years ago. This paper provides an in-depth description of our large-scale Web search engine — the first such detailed public description we know of to date. Apart from the problems of scaling traditional search techniques to data of this magnitude, there are new technical challenges involved with using the additional information present in hypertext to produce better search results. This paper addresses this question of how to build a practical large-scale system which can exploit the additional information present in hypertext. Also we look at the problem of how to effectively deal with uncontrolled hypertext collections where anyone can publish anything they want.}
}

@book{bird2009natural,
  title={Natural language processing with Python: analyzing text with the natural language toolkit},
  author={Bird, Steven and Klein, Ewan and Loper, Edward},
  year={2009},
  publisher={" O'Reilly Media, Inc."}
}

@misc{zhang2020bertscoreevaluatingtextgeneration,
  title={BERTScore: Evaluating Text Generation with BERT}, 
  author={Tianyi Zhang and Varsha Kishore and Felix Wu and Kilian Q. Weinberger and Yoav Artzi},
  year={2020},
  eprint={1904.09675},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/1904.09675}, 
}

@misc{reimers2019sentencebertsentenceembeddingsusing,
  title={Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks}, 
  author={Nils Reimers and Iryna Gurevych},
  year={2019},
  eprint={1908.10084},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/1908.10084}, 
}

@inproceedings{wolf-etal-2020-transformers,
  title = "Transformers: State-of-the-Art Natural Language Processing",
  author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and others",
  booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
  month = oct,
  year = "2020",
  address = "Online",
  publisher = "Association for Computational Linguistics",
  url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",
  pages = "38--45"
}

@article{WANG20254058,
  title = {A percolation phase transition controls complement protein coating of surfaces},
  journal = {Cell},
  volume = {188},
  number = {15},
  pages = {4058-4073.e25},
  year = {2025},
  issn = {0092-8674},
  doi = {https://doi.org/10.1016/j.cell.2025.05.026},
  url = {https://www.sciencedirect.com/science/article/pii/S0092867425005768},
  author = {Zhicheng Wang and Sahil Kulkarni and Jia Nong and Marco Zamora and Alireza Ebrahimimojarad and Elizabeth Hood and Tea Shuvaeva and Michael Zaleski and Damodar Gullipalli and Emily Wolfe and others},
  keywords = {systems biology, immunology, complexity science, complement, host response, biomaterials, nanomedicine},
  abstract = {Summary
  When a material enters the body, it is immediately attacked by hundreds of proteins, organized into complex networks of binding interactions and reactions. How do such complex systems interact with a material, “deciding” whether to attack? We focus on the complement system of ∼40 blood proteins that bind microbes, nanoparticles, and medical devices, initiating inflammation. We show a sharp threshold for complement activation upon varying a fundamental material parameter, the surface density of potential complement attachment points. This sharp threshold manifests at scales spanning single nanoparticles to macroscale pathologies, shown here for diverse engineered and living materials. Computational models show these behaviors arise from a minimal subnetwork of complement, manifesting percolation-type critical transitions in the complement response. This criticality switch explains the “decision” of a complex signaling network to interact with a material.}
}

@misc{xu2025evaluatingsmalllanguagemodels,
  title={Evaluating Small Language Models for News Summarization: Implications and Factors Influencing Performance}, 
  author={Borui Xu and Yao Chen and Zeyi Wen and Weiguo Liu and Bingsheng He},
  year={2025},
  eprint={2502.00641},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2502.00641}, 
}

@misc{muennighoff2025scalingdataconstrainedlanguagemodels,
  title={Scaling Data-Constrained Language Models}, 
  author={Niklas Muennighoff and Alexander M. Rush and Boaz Barak and Teven Le Scao and Aleksandra Piktus and Nouamane Tazi and Sampo Pyysalo and Thomas Wolf and Colin Raffel},
  year={2025},
  eprint={2305.16264},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2305.16264}, 
}

@misc{dorfner2024biomedicallargelanguagesmodels,
  title={Biomedical Large Languages Models Seem not to be Superior to Generalist Models on Unseen Medical Data}, 
  author={Felix J. Dorfner and Amin Dada and Felix Busch and Marcus R. Makowski and Tianyu Han and Daniel Truhn and Jens Kleesiek and Madhumita Sushil and Jacqueline Lammert and Lisa C. Adams and Keno K. Bressem},
  year={2024},
  eprint={2408.13833},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2408.13833}, 
}

@article{zhou2025internal,
  title   = {Investigating and Mitigating Catastrophic Forgetting in Medical Knowledge Injection through Internal Knowledge Augmentation Learning},
  author  = {Zhou, Yuxuan and Liu, Xien and Zhang, Xiao and Ning, Chen and Wang, Shijin and Hu, Guoping and Wu, Ji},
  journal = {OpenReview},
  year    = {2025},
  note    = {Preprint. Available online: \url{https://openreview.net/forum?id=i9RDDi2SZC}}
}

@misc{jin2025reasoningnotcomprehensiveevaluation,
  title={Reasoning or Not? A Comprehensive Evaluation of Reasoning LLMs for Dialogue Summarization}, 
  author={Keyan Jin and Yapeng Wang and Leonel Santos and Tao Fang and Xu Yang and Sio Kei Im and Hugo Gonçalo Oliveira},
  year={2025},
  eprint={2507.02145},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2507.02145}, 
}


@misc{irugalbandara2024scalingscaleupcostbenefit,
  title={Scaling Down to Scale Up: A Cost-Benefit Analysis of Replacing OpenAI's LLM with Open Source SLMs in Production}, 
  author={Chandra Irugalbandara and Ashish Mahendra and Roland Daynauth and Tharuka Kasthuri Arachchige and Jayanaka Dantanarayana and Krisztian Flautner and Lingjia Tang and Yiping Kang and Jason Mars},
  year={2024},
  eprint={2312.14972},
  archivePrefix={arXiv},
  primaryClass={cs.SE},
  url={https://arxiv.org/abs/2312.14972}, 
}