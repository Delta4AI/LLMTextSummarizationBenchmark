
  \acrodef{LM}{language model}
  \acrodef{LLM}{large language model}
  \acrodef{SLM}{small language model}

  \acrodef{EDM}{encoder-decoder model}
  \acrodef{NLP}{natural language processing}
  \acrodef{ATS}{Automatic text summarization}
  \acrodef{TF-IDF}{term frequency-inverse document frequency}
  \acrodef{Seq2seq}{sequence-to-sequence}
  \acrodef{RNN}{recurrent neural networks}
  \acrodef{LSTM}{long short-term memory}
  \acrodef{GRU}{gated recurrent unit}
  \acrodef{CPT}{continual pre-training}
  \acrodef{SFT}{supervised fine-tuning}
  \acrodef{RL}{reinforcement learning}
  \acrodef{API}{Application Programming Interface}
  \acrodef{ANOVA}{Analysis of Variance}

  \acrodef{BERT}{bidirectional encoder representations from transformers}
  \acrodef{BART}{bidirectional and auto-regressive transformer}
  \acrodef{T5}{text-to-text transfer transformer}
  \acrodef{PEGASUS}{Pre-training with extracted gap-sentences for abstractive summarization sequence-to-sequence}
  \acrodef{GPT}{generative pre-trained transformer}
  \acrodef{Llama}{large language model Meta AI}
  \acrodef{LED}{longformer encoder-decoder}

  \acrodef{ROUGE}{recall-oriented understudy for gisting evaluation}
  \acrodef{BLEU}{bilingual evaluation understudy}
  \acrodef{METEOR}{metric for evaluation of translation with explicit ordering}
  \acrodef{RoBERTa}{Robustly optimized BERT approach}
  \acrodef{DeBERTa}{decoding-enhanced BERT with disentangled attention}
  \acrodef{MPNet}{Masked and Permuted Pre-training}