\section{Discussion}

The benchmarking analysis revealed clear performance differences between the evaluated summarization approaches. Overall, general-purpose LLMs achieved the highest summarization quality across all surface-level and embedding-based metrics, followed closely by general-purpose SLMs and reasoning-oriented LLMs. In contrast, domain-specific models, encoder–decoder architectures such as T5 and PEGASUS, and traditional extractive methods like TextRank all reached noticeably lower performance levels. These results highlight the clear progression from extractive and encoder–decoder approaches toward transformer-based models, while also showing that domain-specific fine-tuning alone does not necessarily lead to improved summarization quality.

\subsection{Model Group Comparisons}

To understand the causes of the observed performance differences, the models were compared by architecture, size, and domain focus. This analysis examines how model scale, reasoning ability, and domain specialization influence summarization quality in biomedical texts. The next sections discuss these aspects in detail by comparing large and small language models, general-purpose and domain-specific models, and general-purpose and reasoning-oriented models, referring to Figure~\ref{fig:rank_heatmap}.

\subsubsection{General-purpose Large vs. Small Language Models (LLMs vs. SLMs)}

By comparing the overall performance of SLMs and LLMs, even among the top 10 of the best-performing models the majority are LLMs. This is likely due to their huge number of parameters which allow them to better understand the complex context typical for biomedical literature. While very small models, like Gemma3:270M, can indeed lack the capacity to handle this complexity, SLMs remain competitive, with some models even outperforming certain LLMs. This may be because smaller datasets are often more curated and of higher quality compared to the large amount of data required to train a big model \cite{gunasekar2023textbooksneed}. 

Interestingly, medium-sized models in the range of approx. 20-70B parameters (e.g the Mistral family), appear to be more performant than larger proprietary ones. These models seem to reach an optimal compromise about number of parameters and overall performance where additional parameters could disrupt this equilibrium, leading to potentially over‑fitting or plateauing performance \cite{largerandmoreinstructable}.

\subsubsection{General-purpose vs. Domain-specific Models}

As discussed in the introduction section, domain-specific models have been developed over time with the aim of improving model ability for a specific task. However, in this article, we found that overall general-purpose models outperform both domain-specific models specialized in the biomedical domain and the one specialized for text summarization, regardless of model size. A possible explanation for this behavior is that domain-specific models fine-tuned on biomedical text, might be better for learning and understanding the complex biomedical terminology or lexical patterns but might fail in summarization tasks. On the other hand, models specifically designed for text summarization might be good at summarizing in general but fail at capturing the complex biomedical meaning. That is why generalist models, leveraging their broad knowledge, seem to perform better \cite{dorfner2024biomedicallargelanguagesmodels}. Additionally, domain-specific models can “forget” the general knowledge that was acquired during the pre-training phase, experiencing a phenomenon called “catastrophic forgetting”, which represents an issue when the task requires both domain-specific knowledge, the biomedical knowledge, and context understanding, for text summarization \cite{zhai2023investigatingcatastrophicforgettingmultimodal}.

\subsubsection{General-purpose vs Reasoning-oriented Models}

Even though some reasoning-oriented models ranked among the top 10 of the best performing models, most of them were positioned in the middle of the ranking as moderate performing models. This can be explained by the intrinsic multi-step logical reasoning nature of these models, that while it can be advantageous for tasks that require breaking problems down into sequential steps like for mathematics or coding, it can be not ideal for text summarization that require semantic compression and factual grounding instead \cite{jin2025reasoningnotcomprehensiveevaluation}.

\subsection{Evaluation and Metric Considerations}

The evaluation framework combined surface-level, embedding-based, and factual consistency metrics to capture complementary aspects of summarization quality. Surface-level metrics such as ROUGE, BLEU, and METEOR primarily measure lexical overlap with the reference summaries, while embedding-based metrics including RoBERTa, DeBERTa, and MPNet assess semantic similarity and paraphrasing ability. AlignScore adds a distinct perspective by evaluating factual consistency between the generated summary and its source abstract. Unlike other metrics, it directly compares the summary with the input rather than with the human-written Highlights section. This design enables AlignScore to evaluate factual faithfulness to the source material instead of measuring the similarity to the reference summary.

While the correlation analysis indicated broad agreement among most metrics, AlignScore showed weaker alignments with the others, which emphasizes that factual consistency represents a distinct dimension of summarization quality. The strong performance of extractive approaches such as the frequency-based and TextRank models illustrates this difference: by retaining sentences from the abstract almost verbatim, these models naturally preserve factual accuracy and therefore achieve high AlignScore values, despite weaker results on other metrics.

Nevertheless, using AlignScore in this benchmark was intentional as factual grounding is an essential requirement in scientific text summarization. Models that generate fluent or semantically similar summaries may still introduce factual inaccuracies or exclude key information. Including AlginScore therefore ensures that the benchmark considers both linguistic quality and factual reliability.

\subsection{Model Access Methods and API Heterogeneity}

Model access methods varied across the evaluation due to differing API capabilities and requirements. HuggingFace models were accessed through their supported interfaces: the pipeline API (task="summarization") where available, or chat/completion formats for models that did not support the pipeline approach. Ollama models required use of the generate endpoint with merged prompts, while OpenAI, Anthropic, and Mistral models each mandated their respective provider-specific APIs (responses.create, messages.create, and chat.complete) with distinct message structures. We applied hyperparameter normalization where possible, though API-level constraints prevented full standardization. For example, GPT-5 does not support temperature control, instead offering only reasoning-specific parameters. Additionally, proprietary middleware layers may transform requests and responses in undocumented ways, potentially affecting outputs independently of the underlying model architectures. These necessary methodological variations warrant consideration when interpreting performance differences across models.

\subsection{Limitations and Future Work}

This benchmark focuses on a single summarization task: generating concise summaries from biomedical abstracts. This setup provides a clear and well-defined evaluation framework, but the findings may not fully extend to other forms of scientific or biomedical summarization, including full-length articles, clinical trial data, or lay-oriented summaries. The rapid progress in LLMs means these results reflect a specific snapshot in time and may change as newer architectures and models become available.

Another limitation lies in the exclusive reliance on automatic evaluation metrics. Although combining surface-level, embedding-based, and factual measure offers a broad view, human assessment would provide a more nuanced understanding of readability, coherence, and factual correctness. Future work could therefore extend this benchmark by integrating expert-based evaluations, exploring alternative summarization tasks, and including emerging model families as they are released.

\subsection{Practical Implications and Applications}

The results of this benchmark provide useful guidance for selecting summarization models in biomedical and scientific settings. The strong performance of general-purpose LLMs indicates that broad, diverse pretraining is often more advantageous than narrow domain adaptation when dealing with unseen scientific content.

Another key consideration is the trade-off between output quality and processing efficiency. While LLMs achieved the highest overall scores, SLMs deliver competitive results at substantially lower computational cost, which makes them especially attractive for large-scale or resource-constrained applications. Choosing between large and small models therefore depends not only on desired output quality but also on the intended scale of summarization.

Overall, the findings suggest that general-purpose LLMs currently offer the most reliable and practical choice for biomedical summarization. Their consistent performance across evaluation criteria demonstrates that broad generalization outweighs the marginal gains from more narrowly specialized or fine-tuned approaches, many of which are not primarily optimized for summarization.