\section{Materials and Methods}

\subsection{Gold-Standard Dataset}

To establish a reliable benchmark for automatic summarization, we assembled a gold-standard dataset of 1,000 biomedical articles drawn from a diverse set of peer-reviewed journals hosted on \textit{ScienceDirect} and \textit{Cell Press}. These journals were selected because, in addition to their focus on molecular and biomedical sciences, they provide a standardized \textit{Highlights} section. This section provides concise bullet points that capture the main findings of each article. These served as the reference summaries in our evaluation, while the corresponding abstracts were used as the input texts for the summarization.

Articles were collected systematically across a variety of journals to ensure coverage of different fields within molecular sciences such as drug discovery, genomics, proteomics, biotechnology, and biochemistry. We selected 50 articles from each of the 20 journals, bringing the dataset to 1,000 in total. The distribution of articles across journals is summarized in Table~\ref{tab:goldstandard}.

\begin{table}[H]
\centering
\caption{Overview of journals and number of articles included in the gold-standard dataset.\label{tab:goldstandard}}
\begin{tabularx}{\textwidth}{llc}
\toprule
\textbf{Publisher} & \textbf{Journal} & \textbf{Articles included} \\
\midrule
ScienceDirect & Drug Discovery Today & 50 \\
ScienceDirect & Journal of Molecular Biology & 50 \\
ScienceDirect & FEBS Letters & 50 \\
ScienceDirect & Journal of Biotechnology & 50 \\
ScienceDirect & Gene & 50 \\
ScienceDirect & Genomics & 50 \\
ScienceDirect & Journal of Proteomics & 50 \\
ScienceDirect & The International Journal of Biochemistry \& Cell Biology & 50 \\
ScienceDirect & Cytokine & 50 \\
ScienceDirect & Developmental Cell & 50 \\
Cell & Cell & 50 \\
Cell & Cancer Cell & 50 \\
Cell & Cell Chemical Biology & 50 \\
Cell & Cell Genomics & 50 \\
Cell & Cell Host \& Microbe & 50 \\
Cell & Cell Metabolism & 50 \\
Cell & Cell Reports & 50 \\
Cell & Cell Reports Medicine & 50 \\
Cell & Cell Stem Cell & 50 \\
Cell & Cell Systems & 50 \\
\bottomrule
\end{tabularx}
\end{table}

This setup provides standardized pairs of abstracts and reference summaries that can be directly used for evaluating automatic summarization methods.

\subsection{Summarization Methods}

We evaluated 50 summarization methods, ranging from simples frequency-based algorithms to state-of-the-art large language models (LLMs). By having this extensive coverage of methods, we were able to compare established techniques with the latest transformer-based mdodels under identical conditions.

The models were grouped into five categories:

\begin{enumerate}
	\item Traditional methods: As a foundation for comparison, we included two traditional extractive methods: a simple frequency-based approach and TextRank. These methods provide a simple baseline to compare the more complex approaches with.
	\item Encoder-Decoder models: We included a set of pretrained encoder-decoder models, which are available through the HuggingFace library: BART (base and large), T5 (base and large), mT5, a variety of PEGASUS models, and LED. These models are often applied for abstractive summarization and represent well-established neural systems within our benchmark.
	\item General-purpose LLMs: <TBD, still have to decide on the final groupings>
	\item Reasoning-oriented LLMs: <TBD, still have to decide on the final groupings>
	\item Specialized LLMs: To assess whether domain adapatation improves summarization quality, we included large language models additionally trained on medical/biomedical data, such as MedLlama2 and other specialized models.
\end{enumerate}

The complete list of models included in each category is shown in Table~\ref{tab:models}.

\begin{table}[H]
\caption{Overview of summarization methods/models evaluated in this study, organized by category.\label{tab:models}}
\begin{tabularx}{\textwidth}{lX}
\toprule
\textbf{Group} & \textbf{Methods/Models} \\
\midrule
Traditional methods & textrank; frequency \\
\midrule
HuggingFace transformers & bart-large-cnn; bart-base; t5-base; t5-large; mT5\_multilingual\_XLSum; pegasus-xsum; pegasus-large; pegasus-cnn\_dailymail; led\_large\_16384\_arxiv\_summarization \\
\midrule
Specialized LLMs & medllama2:7b; openbiollm-llama-3:8b\_q8\_0 \\
\midrule
General-purpose LLMs & gemma3:1b; gemma3:4b; gemma3:12b; granite3.3:2b; granite3.3:8b; llama3.1:8b; llama3.2:1b; llama3.2:3b; mistral:7b; mistral-nemo:12b; mistral-small3.2:24b; gemma3-tools:4b; phi3:3.8b; phi4:14b; qwen3:4b; qwen3:8b; gpt-3.5-turbo; gpt-4.1; gpt-4.1-mini; gpt-4o; gpt-4o-mini; claude-3.5-haiku; claude-sonnet-4; mistral-medium; mistral-small; mistral-large; gpt-5-nano; gpt-5-mini \\
\midrule
Reasoning-oriented LLMs & deepseek-r1:1.5b; deepseek-r1:7b; deepseek-r1:8b; deepseek-r1:14b; gpt-oss:20b; claude-opus-4; gpt-5; magistral-medium; claude-opus-4-1; \\
\bottomrule
\end{tabularx}
\end{table}

With this selection we covered models of different sizes and release periods, ensuring that both widely adopted systems and recent architectures were represented. Extraordinarily large models were not considered as they are impractical for typical summarization pipelines and fall outside the scope of our benchmarking goals.

These 50 diverse models were all tasked with generating summaries for each of the 1,000 abstracts in the dataset, resulting in 50,000 generated summaries up for evaluation.

\subsection{Evaluation Metrics}

As there is no single metric that can fully reflect summary quality, especially in the biomedical field where both coverage of key information and factual correctness are critical, we used a multitude of metrics grouped into five categories: traditional surface-level metrics, embedding-based similarity metrics, content coverage metrics, factuality metrics and performance-related measures that reflect the feasibility of using the methods in actual real-world applications. By combining all these metrics into one final overall score, we end up with a balanced benchmark value that reflects both summary quality and practical usability.

\subsubsection{Surface-level Metrics}

This group consists of metrics that compare the generated summaries with the reference summaries mainly at the word or phrase level. While they do not capture meaning beyond surface overlap, they remain common metrics in summarization research and provide a simple foundation for evaluation. We used three ROUGE variants (ROUGE-1, ROUGE-2, ROUGE-L), BLEU and METEOR. ROUGE-1 and ROUGE-2 measure how many unigrams (single words) or bigrams (word pairs) from the reference appear in the generated output, while ROUGE-L identifies the longest sequence of words shared between the two. BLEU calculates how many n-grams in the output also occur in the reference, but it emphasizes precision rather than recall and applies a brevity penalty to counteract the tendency toward overly short summaries. METEOR extends n-gram matching by also considering word stems and synonyms, which makes it more tolerant to variations in wording. Together, these metrics offer a simple but transparent point of reference.

\subsubsection{Embedding-based Similarity Metrics}

To capture similarity beyond surface-level word overlap, we also included embedding-based metrics built on pretrained transformer models. These methods generate vector representations of the texts, which allows them to capture similarity in meaning rather than just word overlap. Specifically, we employed RoBERTa and DeBERTa, two transformer-based models that have shown strong performance on a variety of natural language processing (NLP) tasks. In the case of summarization evaluation, they can be used to judge whether two summaries capture the same content even if phrased differently.

\subsubsection{Content Coverage Metrics}

<TBD, maybe also add to Embedding-based Similarity Metrics?>

\subsubsection{Factuality Metrics}

For factual consistency we used AlignScore, a metric designed to assess whether the statements in a generated summary are supported by the source text. In contrast to the other metrics, we used AlignScore in a way where it does not compare the output to the reference summary but instead aligns it directly with the abstract, as factual correctness can only be judged relative to the input text itself and not against a condensed reference. … By adding AlignScore we have a metric that is sensitive to errors and halucinations…

\subsubsection{Performance Metrics}

\subsection{Benchmarking Framework}
<TBD>

\subsection{Computational Resources}
<TBD>
