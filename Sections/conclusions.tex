\section{Conclusion}

This benchmark provides a comprehensive and up-to-date evaluation of 62 text summarization methods on a dataset of 1,000 biomedical abstracts paired with standardized highlights sections. Across all lexical overlap, semantic similarity, and factual consistency metrics, general-purpose \acp{LLM} demonstrated the most consistent performances. Their ability to incorporate broad pretraining with strong semantic understanding, enabled them to outperform reasoning-oriented models, domain-specific models, and encoder-decoder architectures. Especially medium-sized models achieved the highest overall rankings, which indicates that increased scale beyond this range does not automatically translate to improved summarization quality for biomedical text.

Also \acp{SLM} remain competitive for settings where computational resources are limited as they offer balance between efficiency and output quality. In contrast, domain-specific models did not show a systematic advantage, showing that narrow fine-tuning alone is insufficient to match the generalization of broadly trained \acp{LLM} for this task.

Overall, this benchmark provides a systematic reference point for selecting summarization models in biomedical research. By assessing a broad spectrum of architectures under a unified framework we underline the strengths and limitations of currently available systems and highlight the superiority of general-purpose \acp{LLM} for scientific text summarization. These insights can support researchers and practitioners in choosing models that balance output quality, computational cost, and practical usability in biomedical workflows.
