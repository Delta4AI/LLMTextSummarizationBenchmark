\section{Introduction}

The exponential growth of scientific literature has created a demand for text summarization methods to support scientists in finding relevant information efficiently. Automatic text summarization (ATS) methods have evolved from statistical approaches to deep learning-based models, becoming increasingly sophisticated and reliable at capturing essential parts from complex research articles. ATS methods have been previously evaluated and described \cite{zhang2025comprehensivesurveyprocessorientedautomatic,zhang2024systematicsurveytextsummarization}, but few are tailored for scientific literature summarization \cite{ROHIL2022100058,xie2023surveybiomedicaltextsummarization}. 

The pre-neural era of text summarization was mainly characterized by extractive approaches, where in an unsupervised way, summaries were generated by using word or concept frequencies to identify relevant sentences. The first word-frequency based approaches were discussed by Luhn \cite{Luhn1958TheAC}, who presented a method based on the assumption that recurrent words in a text are likely more important. Later, Edmunson \cite{10.1145/321510.321519} introduced concepts such as cue words, title words, and sentence position to further enhance the automatic summarization process. The concept of Term Frequency–Inverse Document Frequency (TF-IDF) was later adopted \cite{article} and applied to text summarization by representing sentences as term-weight vectors that down-weight common terms and on the other hand up-weight rare terms that might be of more relevance. Thus, word-frequency based approaches have been extensively used in scientific text summarization, being at the basis of more sophisticated strategies \cite{10.1145/1183614.1183701}. Lastly, graph-based methods were implemented, where sentences were represented as nodes and relations between sentences, calculated by using similarity measures (i.e cosine similarity of TF-IDF vectors), as edges. Two graph-based methods gained popularity in the biomedical domain: TextRank, that builds a graph by breaking down the documents into single sentences to then apply the PageRank algorithm to assign importance scores to sentences. The summary is then generated by using the top-ranked sentences \cite{mihalcea-tarau-2004-textrank, shang2014learning}. LexRank, instead, uses eigenvector centrality to find the most influenctial ones in the graph \cite{Erkan_2004}. 

With the advent of Sequence-to-Sequence (Seq2seq) frameworks, summarization shifted toward neural approaches that paraphrase and condense text using Encoder-Decoder architectures, originally implemented with Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, and Gated Recurrent Units (GRUs) \cite{afzal2020, almasoud2022}. The introduction of self-attention mechanisms replaced RNNs by processing sequences in parallel rather than sequentially, enabling the capture of complex linguistic patterns and long-range contextual relationships \cite{vaswani2023attentionneed}. This innovation laid the foundation for transformer architectures that quickly gained popularity in performing a wide range of Natural Language Processing (NLP) tasks, including text summarization. One of the earliest and most influential transformer-based models, Bidirectional Encoder Representations from Transformers (BERT) \cite{devlin2019bertpretrainingdeepbidirectional}, was widely adopted in domain specific tasks thanks to the possibility of fine-tuning by adding a task-specific output layer. Inspired by BERT's architecture, several abstractive summarization models emerged, including Bidirectional and Auto-Regressive Transformer (BART) - a denoising autoencoder for pretraining sequence-to-sequence models \cite{DBLP:journals/corr/abs-1910-13461} that can be trained or fine-tuned on scientific literature \cite{yuan-etal-2022-biobart, abinaya2024}. The Text-to-Text Transfer Transformer (T5) model was introduced as a unified text-to-text framework for a broad spectrum of NLP tasks due to its high flexibility with no need for architectural changes \cite{raffel2023exploringlimitstransferlearning}. Pre-training with Extracted Gap-sentences for Abstractive Summarization Sequence-to-sequence (PEGASUS), was specifically proposed for abstractive summarization \cite{zhang2020pegasuspretrainingextractedgapsentences} and has been adapted for scientific text with domain-specific variants including “google/pegasus-pubmed” and “google/bigbird-pegasus-large-pubmed”. Robustly Optimized BERT Approach (RoBERTa) is an optimized version of BERT trained on a bigger corpus of text, which led to the creation of Longformer \cite{Beltagy2020Longformer}, a transformer-based architecture that can handle longer texts for text-to-text generation, with “allenai/led-base-16384” and “led-large-16384-arxiv” as notable examples \cite{steblianko2024}. Despite these advances, the field of ATS quickly moved towards decoder-only architectures which are at the basis of LLMs, able to capture semantic relations with higher flexibility and specificity. LLMs can be classified as (i) general-purpose models, which leverage their broad domain knowledge across diverse NLP tasks, (ii) reasoning-oriented models, characterized by logical text understanding through iterative chain-of-thought processing and instruction tuning \cite{plaat2025multistepreasoninglargelanguage}, and (iii) domain-specific models, tailored for specialized tasks or scientific domains. Several families of LLMs have been developed, including the GPT series developed by OpenAI (GPT-1 \cite{Radford2018ImprovingLU} through GPT-5) and open-source variants like GPT:OSS, all pre-trained on large-scale text corpora through self-supervised learning. Similarly, Anthropic's Claude Models are built on transformer architecture and trained through a Constitutional AI approach \cite{bai2022constitutionalaiharmlessnessai}. This family also includes a series of reasoning models such as Sonnet-4 and Opus-4. Meta's Llama family, with LLaMA 3.1 as the most capable open-source model available to date, includes domain-specific adaptations such as OpenBioLLM-LLaMA-3, a biomedical variant trained on a large corpus of high-quality biomedical data, and MedLLaMA-2, a medical language model based on LLaMA 2 architecture. Google developed a series of lightweight models including the Gemma series \cite{gemmateam2025gemma3technicalreport}, with Gemma3 as its latest and most powerful reasoning model. Microsoft introduced the Phi series, which comprises Phi-4-reasoning and Phi-4-mini-reasoning, alongside BioGPT \cite{turbitt-etal-2023-mdc}, a domain-specific model built on the GPT architecture and fine-tuned for biomedical applications. IBM released the Granite series, with Granite 4.0 as its reasoning-capable variant. Mistral AI developed the Mistral family, including Magistral as its first reasoning model, and Biomistral, an open-source variant pretrained on PubMed Central data for biomedical text processing. Alibaba Cloud introduced the Qwen 3 series as an open-source LLM family, which inspired SciLitLLM, a specialized model for scientific literature understanding based on Qwen2.5 and trained through continual pre-training (CPT) and supervised fine-tuning (SFT) on scientific literature \cite{li2025scilitllmadaptllmsscientific}. DeepSeek has developed reinforcement learning (RL)-driven reasoning models that achieve performance comparable to state-of-the-art closed-source models while requiring only a fraction of their training costs \cite{wang2025reviewdeepseekmodelskey}. Lastly, APERTUS represents Switzerland’s first large-scale open, multilingual language model with a fully documented and openly accessible development process. 

To the best of our knowledge, no comprehensive benchmarking of text summarizations models on biomedical literature has been performed to date. This study addresses this gap by systematically evaluating 62 summarization models, ranging from classical approaches to state-of-the-art LLMs, on a curated dataset of 1,000 biomedical abstracts with corresponding highlight sections as reference studies. By identifying the strengths and limitations of each approach, we provide actionable insights for selecting appropriate summarization tools to accelerate knowledge discovery in biomedical sciences.  