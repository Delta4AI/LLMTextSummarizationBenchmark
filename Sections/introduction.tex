\section{Introduction}

With the exponential growth of publicly available data, the effort to properly select relevant information has increased dramatically, leading to a scenario of information overloading in which important data may remain hidden. This has created a need for the development of reliable tools that can efficiently generate high-level summaries to highlight only the essential parts. Particularly, in the scientific field, where finding the right content is crucial for generation of novel hypothesis. To address this necessity, automatic text summarization (ATS) methods have undergone significant advances over time, enhancing their reliability in accurately summarizing relevant parts of complex research articles. While the history of ATS has been extensively evaluated and described by several articles \cite{zhang2025comprehensivesurveyprocessorientedautomatic,zhang2024systematicsurveytextsummarization}, only few of them are tailored on scientific literature summarization \cite{ROHIL2022100058,xie2023surveybiomedicaltextsummarization}. This introduction, therefore, aims to cover all the methodological improvements in ATS, ranging from early statistical approaches to modern large language models (LLM), by linking each method to its scientific application. 

\subsection{Pre-Neural era: from word-frequency and early statistical methods to graph based approaches}

Pre-neural era of summarization of scientific literature was characterized mainly by extractive approaches, where in an unsupervised way, summaries were generated by using word or concept frequency to identify relevant sentences. The first word-frequency based approaches were discussed in the Luhn’s paper \cite{Luhn1958TheAC},which presented a method based on the assumption that recurrent words in a text are likely more important and Edmunson \cite{10.1145/321510.321519}, who introduce concepts as cue words, title words, and sentence position to enhance the automatic summarization process. Later, the TD-IDF method, Term Frequency–Inverse Document Frequency, was developed \cite{article}. This approach was applied to text summarization, where sentences were represented by term-weight vectors, with weights assigned to words to down-weight common biomedical terms that are frequent but less relevant, and up-weighting rare terms that might be more relevant. Thus, word-frequency based approaches have been extensively adopted in scientific text summarization, being at the basis of more sophisticated strategies \cite{10.1145/1183614.1183701}. Afterwards, graph-based methods were adopted, where sentences were represented as nodes and relations between sentences, calculated by using similarity measures (i.e cosine similarity of TF-IDF vectors), as edges. Two Graph-based methods gained popularity in the biomedicals domain: i) TextRank, that build a graph by breaking down the documents into single sentences and then exploit the PageRank algorithms to assign importance to each sentence, ultimately building the summary by using the top ranked ones \cite{mihalcea-tarau-2004-textrank, shang2014learning} and ii) LexRank that instead, use eigenvector centrality to find the ones that are most influential in the graph. \cite{Erkan_2004} 

\subsection{Neural network era: from sequence-to-sequence frameworks to modern LLM}

With the advent of Sequence-to-Sequence framework (Seq2seq), summaries were generated by paraphrasing and condensing the text. This involves the use of an Encoder-Decoder architecture, originally implemented as Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks and Gated Recurrent Units (GRUs) and then adapted to the biomedical domain \cite{afzal2020, almasoud2022}. Later, the introduction of self-attention mechanisms led to the replacement of recurrent networks, able to process sequencies in a sequential way, in favours of self-attention mechanisms, that handle entire sequence in parallel, capturing more complex linguistics patterns and long context relationship \cite{vaswani2023attentionneed}. This approach is at the basis of Transformer that gained popularity for performing different biomedical Natural Language Processing (NLP). One of the earliest language representation models, was the BERT model, a Bidirectional Encoder Representations from Transformers \cite{devlin2019bertpretrainingdeepbidirectional}. This type of architecture has been widely adopted in the biomedical field thanks to the possibility of being fine-tuned by adding a task-specific output layer. Of note, for scientific text summarization BERT was coupled with unsupervised clustering methods to cluster the produced embeddings based on similarity, ultimately ranking the most relevant sentences to generate the summary \cite{moradi2019clusteringdeepcontextualizedrepresentations,liu2019finetunebertextractivesummarization,sefid2022scibertsumextractivesummarizationscientific}. Building on this, models for abstractive summarization inspired by the BERT architecture appeared: i) BART, Bidirectional and Auto-Regressive Transformer, a denoising autoencoder for pretraining sequence-to-sequence models \cite{DBLP:journals/corr/abs-1910-13461}, which can be adapted to the scientific literature by pre-training or fine-tuning the model on biomedical text \cite{yuan-etal-2022-biobart,abinaya2024}. ii) T5, Text-to-Text Transfer Transformer, was introduced as a unified text-to-text framework for different NLP task. The advantage of using this model relies on its flexibility, as there is no need to architectural changes to be used for specific tasks \cite{raffel2023exploringlimitstransferlearning}. iii) PEGASUS, Pre-training with Extracted Gap-sentences for Abstractive Summarization Sequence-to-sequence models, was proposed as a specific model designed for abstractive summarization \cite{zhang2020pegasuspretrainingextractedgapsentences}. Of note, some fine-tuned versions are “google/pegasus-pubmed” and “google/bigbird-pegasus-large-pubmed” that combine BigBird \cite{zaheer2021bigbirdtransformerslonger} and PEGASUS. iiii) Longformer \cite{Beltagy2020Longformer},built on RoBERTa, along its Encoder-Decoder variant for text-to-text generation “allenai/led-base-16384” that was built to handle longer text. Particularly, “led-large-16384-arxiv”, a model fine tuned on arXiv data was developed \cite{steblianko2024}. However, despite these advances, the field of ATS quickly moved towards the use of LLMs, decoder-only transformer architecture, which represent more flexible and performant models in capturing semantic relations. LLMs can be generally classified in i) general-purpose LLMs, which leverage their broad training on different domains dataset to generate abstractive summaries, reasoning- oriented LLMs, which are fine-tuned for multi-step reasoning through advanced chain-of-thought and instruction tuning to increase general understanding \cite{plaat2025multistepreasoninglargelanguage} and domain-specific LLMs, designed to specifically address the challenges of complex biomedical corpus. Several families of LLM have been developed over the years. The OpenAI GPT series models, from the first GPT-1 \cite{Radford2018ImprovingLU} to the recently released reasoning- oriented GPT 5 series (Nano, Mini, Full) and GPT:OSS, are all pre-trained using a huge amount of data in a self-supervised way. This family includes some domain-specific variant as BioGPT \cite{turbitt-etal-2023-mdc}, specifically fine-tuned on the biomedical domain. Similarly, Anthropic Claude Models are built on transformer architecture trained by using an approach called Constitutional AI \cite{bai2022constitutionalaiharmlessnessai} including also some reasoning models: Claude Sonnet-4, Opus-4 and Opus-4-1. The Meta Llama family, with Llama 3 being the most capable model of the family, has also developed some domain-specific adaptations such as OpenBioLLM-Llama-3, a variant of Meta's Llama-3 trained on a large corpus of high-quality biomedical data and medllama2, a medical language model built on Meta’s LLaMA 2 architecture. Google and Microsoft developed a series of lightweight models: the Google Gemma series \cite{gemmateam2025gemma3technicalreport}, which includes Gemma3 as its latest and most powerful reasoning model and the Microsoft Phi series, which comprises Phi-4-reasoning and Phi-4-mini-reasoning. Other models such as Granite 4.0, a reasoning model of the IBM’s Granite series and Magistral, the first reasoning model of Mistral have also been released. Additionally, Mistral developed Biomistral, an open-source model pretrained on PubMed Central data. Moreover, Alibaba Cloud’s introduced the Qwen 3 series as an open-source LLM family, where recently, SciLitLLM has been further developed as a specialized model for scientific literature understanding based on Qwen2.5 and trained through continual pre-training (CPT) and supervised fine-tuning (SFT) on scientific literature \cite{li2025scilitllmadaptllmsscientific}. Lastly, DeepSeek has also developed some reinforcement learning (RL)-driven reasoning models, which are cost-effective and efficient \cite{wang2025reviewdeepseekmodelskey}. 

At the best of our knowledge, no comprehensive peer-reviewed publications that asses the performance of all the discussed methods are available. Therefore, our work, aim at evaluating the efficiency of most of the discussed approaches, highlighting both strengths and limitations of different methods and providing useful insights in using these tools for boosting knowledge discovery in molecular sciences. 
