\section{Introduction}

The exponential growth of scientific literature has created a demand for text summarization methods to support scientists in finding relevant information in an efficient way. Automatic text summarization (ATS) methods, ranging from statistical approaches to modern large language models (LLMs) have matured over time, enhancing their reliability in accurately summarizing relevant parts of complex research articles. ATS methods have been previously evaluated and described \cite{zhang2025comprehensivesurveyprocessorientedautomatic,zhang2024systematicsurveytextsummarization}, but only few of them are tailored for scientific literature summarization \cite{ROHIL2022100058,xie2023surveybiomedicaltextsummarization}. 

The pre-neural era of scientific literature summarization was mainly characterized by extractive approaches, where in an unsupervised way, summaries were generated by using word or concept frequencies to identify relevant sentences. The first word-frequency based approaches were discussed by Luhn \cite{Luhn1958TheAC}, who presented a method based on the assumption that recurrent words in a text are likely more important. Later, Edmunson \cite{10.1145/321510.321519} introduced concepts such as cue words, title words, and sentence position to further enhance the automatic summarization process. The concept of Term Frequency–Inverse Document Frequency (TF-IDF) was later introduced \cite{article} and applied to text summarization by representing sentences as term-weight vectors that down-weight common (biomedical) terms and on the other hand up-weight rare terms that might be of more relevance. Thus, word-frequency based approaches have been extensively adopted in scientific text summarization, being at the basis of more sophisticated strategies \cite{10.1145/1183614.1183701}. Lastly, graph-based methods were adopted, where sentences were represented as nodes and relations between sentences, calculated by using similarity measures (i.e cosine similarity of TF-IDF vectors), as edges. Two graph-based methods gained popularity in the biomedical domain: TextRank, that builds a graph by breaking down the documents into single sentences to then apply the PageRank algorithm to assign importance scores to sentences, ultimately building the summary by using the top-ranked ones \cite{mihalcea-tarau-2004-textrank, shang2014learning}. LexRank as an alternative uses eigenvector centrality to find the most influenctial ones in the graph \cite{Erkan_2004}. 

With the advent of Sequence-to-Sequence (Seq2seq) frameworks, summaries were generated by paraphrasing and condensing text based on Encoder-Decoder architectures, originally implemented as Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks and Gated Recurrent Units (GRUs) \cite{afzal2020, almasoud2022}. The introduction of self-attention mechanisms replaced recurrent networks by processing sequences in parallel rather than sequentially as a more effective method for capturing complex linguistic patterns and long context relationship \cite{vaswani2023attentionneed}. This set the basis for transformer architectures that quickly gained popularity in performing a wide range of Natural Language Processing (NLP) tasks such as text summarization. One of the earliest and most influential transformer-based models, BERT, (Bidirectional Encoder Representations from Transformers) \cite{devlin2019bertpretrainingdeepbidirectional} was widely adopted in domain specific tasks thanks to the possibility to be fine-tuned by adding a task-specific output layer. Inspired by the BERT architecture, different models capable of performing abstractive summarization emerged: BART, Bidirectional and Auto-Regressive Transformer, is a denoising autoencoder for pretraining sequence-to-sequence models \cite{DBLP:journals/corr/abs-1910-13461} that can be trained or fine-tuned on scientific literature \cite{yuan-etal-2022-biobart, abinaya2024}. T5, Text-to-Text Transfer Transformer, was introduced as a unified text-to-text framework for a broad spectrum of NLP tasks due to its high flexibility with no need for architectural changes \cite{raffel2023exploringlimitstransferlearning}. PEGASUS, Pre-training with Extracted Gap-sentences for Abstractive Summarization Sequence-to-sequence model, was specifically proposed for abstractive summarization tasks \cite{zhang2020pegasuspretrainingextractedgapsentences}. Notably, some PEGASUS versions tailored for scientific text summarization were developed such as “google/pegasus-pubmed” and “google/bigbird-pegasus-large-pubmed”. RoBERTa, Robustly Optimized BERT Approach, is an improved version of BERT trained on a bigger corpus of text with some key optimizations, which led to the creation of Longformer \cite{Beltagy2020Longformer}, a transformed based model, that can handle longer texts with its Encoder-Decoder variant for text-to-text generation “allenai/led-base-16384” and “led-large-16384-arxiv” \cite{steblianko2024}. Despite these advances, the field of ATS quickly moved towards decoder-only architectures which are at the basis of LLMs, able to capture semantic relations with higher flexibility and specificity. LLMs can be classified as (i) general-purpose models, which leverage their broad domain knowledge for a variety of NLP tasks, (ii) reasoning-oriented models, characterized by a logical understanding of the text through iterative chain-of-thought processing and instruction tuning \cite{plaat2025multistepreasoninglargelanguage}, and (iii) domain-specific ones, designed to address specific tasks. Several families of LLMs have been developed, from the first GPT-1 \cite{Radford2018ImprovingLU} to the recently released reasoning-oriented GPT-5 series (Nano, Mini, Full) and GPT:OSS, OpenAI GPT series models that are all pre-trained on large-scale text corpora in a self-supervised way. Similarly, Anthropic Claude Models are built on transformer architecture, trained through a Constitutional AI approach \cite{bai2022constitutionalaiharmlessnessai}. This family also includes a series of reasoning models such as Claude Sonnet-4, Opus-4 and Opus-4-1. Meta Llama family, where Llama 3 is the most capable model of the family up to now, comprises some domain-specific adaptations like OpenBioLLM-Llama-3, a variant of Meta's Llama-3 trained on a large corpus of high-quality biomedical data and medllama2, a medical language model built on Meta’s LLaMA 2 architecture. Google and Microsoft developed a series of lightweight models such as Google Gemma series \cite{gemmateam2025gemma3technicalreport}, with Gemma3 as its latest and most powerful reasoning model and the Microsoft Phi series, which comprises Phi-4-reasoning and Phi-4-mini-reasoning. Notably, Microsoft developed BioGPT \cite{turbitt-etal-2023-mdc}, a model built on the GPT architecture and specifically fine-tuned on the biomedical domain. Other models such as Granite 4.0, a reasoning model of the IBM’s Granite series and Magistral, the first reasoning model of the Mistral family have also been released. Remarkably, Mistral developed Biomistral, an open-source model pretrained on PubMed Central data. Moreover, Alibaba Cloud’s introduced the Qwen 3 series as an open-source LLM family, where recently, SciLitLLM has been further developed as a specialized model for scientific literature understanding based on Qwen2.5 and trained through continual pre-training (CPT) and supervised fine-tuning (SFT) on scientific literature \cite{li2025scilitllmadaptllmsscientific}. Lastly, DeepSeek has developed reinforcement learning (RL)-driven reasoning models, which are cost-effective and efficient \cite{wang2025reviewdeepseekmodelskey}. APERTUS, Switzerland’s first large-scale open, multilingual language model has been developed as an open-source model where the entire development process is openly accessible and fully documented. 

To the best of our knowledge, no comprehensive evaluation and benchmarking of the different text summarizations for biomedial texts has been performed so far. The aim of this study was therefore to assess the text summarization performance of a number of models using a generated gold-standard dataset of biomedical abstracts and highlight sections of journals from the biomedical domain. We highlight strengths and limitations of each model to ultimately provide insights for accelerating knowledge discovery in molecular sciences.  